import numpy as np
import numpy.linalg as npla
import toolbox.generaltoolbox as gtb
import toolbox.matrixtoolbox as mtb
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("error")

graphsize=9
font = {"family": "serif",
    "color": "black",
    "weight": "bold",
    "size": "20"}

def simplex(A,b,c,inequalities=None,mode="maximize",inform=False):
    #This method solves the optimization problem
    #"maximize c^Tx subject to Ax<=b and x>=0" using the Simplex Method,
    #assuming the set of feasible solutions is convex. Variable <inform> allows
    #for the user to see diagnostic data of the method.
    
    if not isinstance(A,np.ndarray):
        try:
            A=np.array(A)
        except Exception:
            print("Error: Matrix A of incompatible type.")
            return None
    if not isinstance(b,np.ndarray):
        try:
            b=np.array(b)
        except Exception:
            print("Error: Vector b of incompatible type")
            return None
    if not isinstance(c,np.ndarray):
        try:
            c=np.array(c)
        except Exception:
            print("Vector c of incompatible type")
            return None
    if A.shape[1]!=c.shape[0] or A.shape[0]!=b.shape[0]:
        print("A,b, and c of incompatible sizes")
        return None
    
    n=c.shape[0]
    m=0
    if isinstance(inequalities,(list,tuple,np.ndarray)):
        inequalities=np.array(inequalities)
        for i in range(min(A.shape[0],inequalities.shape[0])):
            try:
                inequalities[i]=inequalities[i].lower()
                if ">" in inequalities[i] or "great" in inequalities[i]:
                    inequalities[i]=">"
                    m+=1
                #elif "=" in inequalities[i] or "equal" in inequalities[i]:
                #    inequalities[i]="="
                    #SLIGHT ERROR: MUST FIRST BE ON A FEASIBLE SOLUTION
                else:
                    inequalities[i]="<"
                    m+=1
            except Exception:
                inequalities[i]="<"
                m+=1
        m+=A.shape[0]-inequalities.shape[0]
        inequalities=np.hstack((inequalities,["<" for i in range(inequalities.shape[0],A.shape[0])]))
    else:
        m=A.shape[0]
        inequalities=["<" for i in range(A.shape[0])]
    
    d=np.hstack((c,np.zeros(m)))
    if isinstance(mode,str):
        if "min" in mode.lower():
            r1=np.hstack((d,0))
        else:
            r1=np.hstack((-d,0))
    else:
        r1=np.hstack((-d,0))
    
    
    A=np.hstack((A,np.zeros((A.shape[0],m))))
    col_count=n
    for i in range(A.shape[0]):
        if inequalities[i]==">":
            A[i,col_count]=-1.0
            col_count+=1
        elif inequalities[i]=="<":
            A[i,col_count]=1.0
            col_count+=1
    r2=np.hstack((A,np.array([b]).T))
    S=np.vstack((r1,r2))
    
    iter_count=0
    pivot=True
    while pivot:
        pivot_col=-1
        max_cost=0
        for j in range(n+m):
            if S[0,j]<max_cost:
                pivot_col=j
                max_cost=S[0,j]
                #break
        if pivot_col==-1:
            pivot=False
            break
        
        pivot_row=-1
        min_ratio=np.inf
        for i in range(1,S.shape[0]):
            if S[i,pivot_col]!=0:
                res=S[i,-1]/S[i,pivot_col]
                if 0<res<min_ratio:
                    pivot_row=i
                    min_ratio=res
        if pivot_row==-1:
            pivot=False
            break
        
        if S[pivot_row,pivot_col]!=1.0:
            S[pivot_row,:]=S[pivot_row,:]/S[pivot_row,pivot_col]
            S[pivot_row,pivot_col]=1.0
        
        for i in range(S.shape[0]):
            if i!=pivot_row and S[i,pivot_col]!=0:
                S[i,:]=S[i,:]-S[pivot_row,:]*S[i,pivot_col]
                S[i,pivot_col]=0.0
        
        iter_count+=1
    
    sol=np.zeros(S.shape[1]-1)
    for j in range(n+m):
        basis_bool=True
        one_pos=-1
        for i in range(1,S.shape[0]):
            if S[i,j]!=0.0:
                if S[i,j]!=1.0:
                    basis_bool=False
                    break
                else:
                    if one_pos!=-1:
                        basis_bool=False
                        break
                    else:
                        one_pos=i
        if basis_bool and one_pos!=-1:
            sol[j]=S[one_pos,-1]
    
    if inform:
        print("Optimal Variable Values:")
        for j in range(n):
            if sol[j]!=0.0:
                print("x_%i = %0.6f"%(j+1,sol[j]))
        print("Optimal Slack Variable Values:")
        for i in range(n,n+m):
            if sol[i]!=0.0:
                print("x_%i = %0.6f"%(i+1,sol[i]))
        print("Maximal Value: %0.6f"%S[0,-1])
        print("Number of Iterations: %i"%iter_count)

    return sol[:n]

def nelder_mead(f,x0,args=[],kwargs={},errtol=1e-6,maxlevel=100,inform=False):
    class simplex(object):
        def __init__(self,V,f):
            self.V=V.copy()
            self.f=f
            self.F=np.array([self.f(v,*args,**kwargs) for v in V])
            
        def order(self):
            index=np.argsort(self.F)
            self.V=self.V[index]
            self.F=self.F[index]
            self.C=np.mean(self.V[:-1],axis=0)
            self.FC=self.f(self.C,*args,**kwargs)
            
        def reflect(self,a=1.0):
            v=self.C+a*(self.C-self.V[-1])
            fv=self.f(v,*args,**kwargs)
            if fv<=self.F[0]:
                self.expand(v,fv)
            elif fv<=self.F[-2]:
                self.V[-1]=v.copy()
                self.F[-1]=fv
            elif fv<self.F[-1]:
                self.contract(v,fv)
            else:
                self.contract(v,fv,inner=True)
        
        def expand(self,vr,fvr,a=2.0):
            v=self.C+a*(self.C-self.V[-1])
            fv=self.f(v,*args,**kwargs)
            if fv<=self.F[0]:
                #self.expand(v,fv,a=2*a)
                self.V[-1]=v.copy()
                self.F[-1]=fv
            else:
                self.V[-1]=vr.copy()
                self.F[-1]=fvr
        
        def contract(self,vr,fvr,a=0.5,inner=False):
            if inner:
                v=self.C-a*(self.C-self.V[-1])
            else:
                v=self.C+a*(self.C-self.V[-1])
            fv=self.f(v,*args,**kwargs)
            if (inner and fv<=self.F[-1]) or (not inner and fv<=fvr):
                self.V[-1]=v.copy()
                self.F[-1]=fv
            else:
                self.shrink()
        
        def shrink(self,a=0.5):
            self.V[1:]=self.V[1:]-a*(self.V[1:]-self.V[0])
            self.F[1:]=[self.f(v,*args,**kwargs) for v in V[1:]]
    
    if ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        x0=np.asarray(x0)
        dim=len(x0)
        try:
            f0=f(x0,*args,**kwargs)
            if ("int" not in type(f0).__name__) and ("float" not in type(f0).__name__):
                if inform:
                    print("Error: unable to apply Nelder-Mead iteration to this function")
                return None
        except Exception:
            if inform:
                print("Error: unable to apply Nelder-Mead iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Nelder-Mead iteration to this function")
        return None
    
    errtol=abs(errtol)
    maxlevel=abs(maxlevel)
    
    V=[x0]
    for i in range(dim):
        res=np.zeros(dim)
        res[i]=1.0
        V.append(x0+res)
    V=np.array(V)
    s=simplex(V,f)
    std=np.std(s.F)
    count=0
    if inform:
        plotline=[f(np.mean(s.V,axis=0),*args,**kwargs)]
    while std>errtol and count<maxlevel:
        s.order()
        s.reflect()
        std=np.std(s.F)
        if inform:
            plotline.append(s.F[0])
        count+=1
    
    if inform:
        print("Solution after %i iterations"%count)
        print("F(x)=%0.6f"%s.F[0])
        ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
        ax.plot([i for i in range(len(plotline))],plotline)
        ax.set_title("Simplex Min Function Value per Iteration",fontdict=font)
        ax.set_xlabel("Iteration",fontsize=16,rotation=0)
        ax.set_ylabel("Function Value",fontsize=16)
        ax.xaxis.set_tick_params(labelsize=16)
        ax.yaxis.set_tick_params(labelsize=16)
    return s.V[0]
    
def exact_line_search(f,start,stop,args=[],kwargs={},errtol=1e-6,maxlevel=100,inform=False):
    #This method performs a line search of a function f:R->R to find the
    #minimum of f between variables start and stop using the Brent minimization
    #method, which itself is a hybrid of Golden Section Search and Jarrat
    #iteration. It is guaranteed to converge, and usually does super-linearly.
    #Variable errtol sets the minimum distance between start and stop before an
    #approximate of the minimum is made. Variable maxlevel determines the
    #maximum number of iterations the method can perform before it is forced to
    #end. If variable inform is set to true, the method relays information on 
    #it's converge behavior to the user. 
    
    errtol=abs(errtol)
    maxlevel=abs(maxlevel)
    
    if stop<start:
        temp=start
        start=stop
        stop=temp
    phi=(1+np.sqrt(5))/2
    x=stop-(phi-1)*(stop-start)
    fx=f(x,*args,**kwargs)
    v=x
    w=x
    fv=fx
    fw=fx
    d=0
    e=0
    m=(start+stop)/2
    count=1
    if inform:
        F=[f(m,*args,**kwargs)]
    while stop-start>errtol and count<maxlevel:
        r=(x-w)*(fx-fv)
        tq=(x-v)*(fx-fw)
        tp=(x-v)*tq-(x-w)*r
        tq2=2*(tq-r)
        if tq2>0:
            p=-tp
            q=tq2
        else:
            p=tp
            q=-tq2
        safe=(q!=0.0)
        if safe:
            try:
                deltax=p/q
            except Exception:
                deltax=0.0
        else:
            deltax=0.0
        parabolic=(safe and (start<x+deltax<stop) and (abs(deltax)<abs(e)/2))
        if parabolic:
            e=d
            d=deltax
        else:
            if x<m:
                e=stop-x
            else:
                e=start-x
            d=(2-phi)*e
        u=x+d
        fu=f(u,*args,**kwargs)
        if fu<=fx:
            if u<x:
                stop=x
            else:
                start=x
            v=w
            w=x
            x=u
            fv=fw
            fw=fx
            fx=fu
        else:
            if u<x:
                start=u
            else:
                stop=u
            if fu<=fw or w==x:
                v=w
                w=u
                fv=fw
                fw=fu
            elif fu<=fv or v==x or v==w:
                v=u
                fv=fu
        m=(start+stop)/2
        if inform:
            F.append(f(m,*args,**kwargs))
        count+=1
    if stop-start>errtol:
        if inform:
            print("Unable to find minimum after %i iterations"%count)
        return None
    else:    
        if inform:
            print("Minimum found after %i iterations"%count)
            print("f(x) = %0.6f"%F[-1])
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Value per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Value",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return m

def line_search(f,x0,delta_x,args=[],kwargs={},c1=1e-4,c2=0.9,maxlevel=100,wolfe_mode=True):
    f0=f(x0)
    a=0.0
    alpha=1.0
    b=np.inf
    bisect_count=0
    cond_flag=False
    grad_f0=gtb.differentiate(f,x0,args=args,kwargs=kwargs)
    if wolfe_mode:
        while not cond_flag and bisect_count<maxlevel:
            try:
                res1=alpha*delta_x
                if f(x0+res1)>f0+c1*res1*grad_f0:
                    b=alpha
                    alpha=0.5*(a+b)
                elif gtb.differentiate(f,x0+res1,args=args,kwargs=kwargs)<c2*grad_f0:
                    a=alpha
                    if b<np.inf:
                        alpha=0.5*(a+b)
                    else:
                        alpha=2*a
                else:
                    cond_flag=True
            except Exception:
                b=alpha
                alpha=0.5*(a+b)
            bisect_count+=1
    else:
        while not cond_flag and bisect_count<maxlevel:
            try:
                res1=alpha*delta_x
                res2=f(x0+res1)
                res3=res1*grad_f0
                if res2>f0+c1*res3:
                    b=alpha
                    alpha=0.5*(a+b)
                elif res2<f0+(1-c1)*res3:
                    a=alpha
                    if b<np.inf:
                        alpha=0.5*(a+b)
                    else:
                        alpha=2*a
                else:
                    cond_flag=True
            except Exception:
                b=alpha
                alpha=0.5*(a+b)
            bisect_count+=1
    return alpha

def multi_line_search(f,x0,delta_x,args=[],kwargs={},c1=1e-4,c2=0.9,maxlevel=100,wolfe_mode=True):
    f0=f(x0)
    a=0.0
    alpha=1.0
    b=np.inf
    bisect_count=0
    cond_flag=False
    grad_f0=mtb.grad(f,x0,args=args,kwargs=kwargs)
    if wolfe_mode:
        while not cond_flag and bisect_count<maxlevel:
            try:
                res1=x0+alpha*delta_x
                res2=np.dot(delta_x,grad_f0)
                if f(res1)>f0+c1*alpha*res2:
                    b=alpha
                    alpha=0.5*(a+b)
                elif np.dot(delta_x,mtb.grad(f,res1,args=args,kwargs=kwargs))<c2*res2:
                    a=alpha
                    if b<np.inf:
                        alpha=0.5*(a+b)
                    else:
                        alpha=2*a
                else:
                    cond_flag=True
            except Exception:
                b=alpha
                alpha=0.5*(a+b)
            bisect_count+=1
    else:
        while not cond_flag and bisect_count<maxlevel:
            try:
                res1=alpha*delta_x
                res2=f(x0+res1)
                res3=np.dot(res1,grad_f0)
                if res2>f0+c1*res3:
                    b=alpha
                    alpha=0.5*(a+b)
                elif res2<f0+(1-c1)*res3:
                    a=alpha
                    if b<np.inf:
                        alpha=0.5*(a+b)
                    else:
                        alpha=2*a
                else:
                    cond_flag=True
            except Exception:
                b=alpha
                alpha=0.5*(a+b)
            bisect_count+=1
    if bisect_count>=maxlevel:
        return 0.0
    return alpha

def newton_Raphson(f,x0,args=[],kwargs={},errtol=1e-6,zerotol=1e-8,maxlevel=100,mode="bad_broyden",adapt=True,inform=False):
    #This method performs a Newton-Raphson approximation of the root of the 
    #function f:R->R, using scalar x0 as it's initial guess.
    #Variable errtol sets the minimum norm that the function can take on before
    #an approximate of the root is made. Variable maxlevel determines the
    #maximum number of iterations the method can perform before it is forced to
    #end. If variable mode is set to newton_raphson, the inverse Jacobian is
    #computed every iteration. If variable mode is set to broyden (default),
    #the inverse Jacobian is approximated using the Broyden method. If variable
    #adapt is set to True, the method uses the weak Wolfe conditions to damp
    #(or accelerate) the Newton-Raphson iteration to coerce global converge to
    #a minimum or root. If variable inform is set to true, the method relays
    #information on it's converge behavior to the user. 
    
    f0=f(x0,*args,**kwargs)
    nf0=abs(f0)
    if ("int" in type(x0).__name__) or ("float" in type(x0).__name__):
        if ("int" not in type(f0).__name__) and ("float" not in type(f0).__name__):
            if inform:
                print("Error: unable to apply Newton-Raphson iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Newton-Raphson iteration to this function")
        return None
        
    mode=mode.lower()
    if "broyden" in mode:
        mode="broyden"
    else:
        mode="newton_raphson"
    errtol=abs(errtol)
    zerotol=abs(zerotol)
    maxlevel=abs(maxlevel)
    
    try:
        dfinv=1.0/gtb.differentiate(f,x0,args=args,kwargs=kwargs)
    except Exception:
        dfinv=0.0
    
    min_flag=(nf0<=errtol)
    extreme_flag=False
    newt_count=0
    if inform:
        F=[nf0]
    
    while not min_flag and not extreme_flag and newt_count<maxlevel:
        delta_x=-dfinv*f0
        alpha=1.0
        if adapt:# and delta_x*gtb.differentiate(f,x0,args=kwargs)*f0<-zerotol:
            if mode=="newton_raphson":
                alpha=line_search(lambda x:abs(f(x,*args,**kwargs)),x0,delta_x)
            else:
                alpha=line_search(lambda x:abs(f(x,*args,**kwargs)),x0,delta_x,wolfe_mode=False)
        x1=x0+alpha*delta_x
        f1=f(x1,*args,**kwargs)
        nf1=abs(f1)
        
        if "broyden" in mode:
            try:
                dfinv=(x1-x0)/(f1-f0)
            except Exception:
                try:
                    dfinv=1.0/gtb.differentiate(f,x1,args=args,kwargs=kwargs)
                except Exception:
                    dfinv=0.0
        else:
            try:
                dfinv=1.0/gtb.differentiate(f,x1,args=args,kwargs=kwargs)
            except Exception:
                try:
                    dfinv=(x1-x0)/(f1-f0)
                except Exception:
                    dfinv=0.0
        
        if inform:
            F.append(nf1)
        min_flag=(nf1<=errtol)
        extreme_flag=(abs(x1-x0)<=zerotol)
        x0=x1
        f0=f1
        nf0=nf1
        newt_count+=1

    if not min_flag and not extreme_flag:
        if inform:
            print("Unable to find extremum or minimum after %i iterations"%newt_count)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return None
    else:
        if inform:
            if min_flag:
                print("Root found after %i iterations"%newt_count)
                print("|f(x)| = %0.6f"%nf0)
            else:
                print("Minumum found after %i iterations"%newt_count)
                print("|f(x)| = %0.6f"%nf0)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return x1

def multi_Newton_Raphson(f,x0,args=[],kwargs={},errtol=1e-6,zerotol=1e-8,maxlevel=100,mode="bad_broyden",adapt=True,inform=False):
    #This method performs a Newton-Raphson approximation of the root of the 
    #function f:R^n->R^n, using vector x0 as it's initial guess.
    #Variable errtol sets the minimum norm that the function can take on before
    #an approximate of the root is made. Variable maxlevel determines the
    #maximum number of iterations the method can perform before it is forced to
    #end. If variable mode is set to newton_raphson, the inverse Jacobian is
    #computed every iteration. If variable mode is set to good_broyden, the
    #inverse Jacobian is approximated using the good Broyden method. If
    #variable mode is set to bad_broyden (default), the inverse Jacobian is 
    #approximated using the bad Broyden method. If variable adapt is set to
    #True, the method uses the weak Wolfe conditions to damp (or accelerate)
    #the Newton-Raphson iteration to coerce global converge to a minimum or root.
    #If variable inform is set to true, the method relays information on 
    #it's converge behavior to the user. 
    
    if ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        x0=np.asarray(x0)
        try:
            f0=f(x0,*args,**kwargs)
            nf0=npla.norm(f0)
        
            if ("list" in type(f0).__name__) or ("tuple" in type(f0).__name__) or ("ndarray" in type(f0).__name__):
                if len(x0)==len(f0):
                    dim=len(x0)
                else:
                    if inform:
                        print("Error: unable to apply Newton-Raphson iteration to f:R^%i->R^%i"%(len(x0),len(f0)))
                    return None
            else:
                if inform:
                    print("Error: unable to apply Newton-Raphson iteration to this function")
                return None
        except Exception:
            if inform:
                print("Error: unable to apply Newton-Raphson iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Newton-Raphson iteration to this function")
        return None
    
    mode=mode.lower()
    if "broyden" in mode:
        if "g" in mode:
            mode="good_broyden"
        else:
            mode="bad_broyden"
        try:
            dfinv=npla.inv(mtb.jacobian(f,x0,args=args,kwargs=kwargs))
        except Exception:
            dfinv=np.zeros(dim,dim)
    else:
        mode="newton_raphson"
        try:
            dfinv=mtb.jacobian(f,x0,args=args,kwargs=kwargs)
        except Exception:
            dfinv=np.zeros(dim,dim)
        
    errtol=abs(errtol)
    zerotol=abs(zerotol)
    maxlevel=abs(maxlevel)
    
    alpha=1.0
    min_flag=(nf0<=errtol)
    extreme_flag=False
    newt_count=0
    if inform:
        F=[nf0]
    while not min_flag and not extreme_flag and newt_count<maxlevel:
        if mode=="newton_raphson":
            delta_x=-npla.solve(dfinv,f0)            
        else:
            delta_x=-np.dot(dfinv,f0)
        if adapt:# and np.inner(f0,np.dot(mtb.jacobian(f,x0,args=args,kwargs=kwargs),delta_x))<-zerotol:
            if mode=="newton_raphson":
                alpha=multi_line_search(lambda X:npla.norm(f(X,*args,**kwargs)),x0,delta_x)
            else:
                alpha=multi_line_search(lambda X:npla.norm(f(X,*args,**kwargs)),x0,delta_x,wolfe_mode=False)
        x1=x0+alpha*delta_x
        f1=f(x1,*args,**kwargs)
        nf1=npla.norm(f1)
        
        if mode=="bad_broyden":
            try:
                dfinv=dfinv+np.outer(((x1-x0)-np.dot(dfinv,f1-f0))/sum((f1-f0)**2),f1-f0)
            except Exception:
                try:
                    dfinv=dfinv+np.outer(((x1-x0)-np.dot(dfinv,f1-f0))/np.dot(x1-x0,np.dot(dfinv,f1-f0)),np.dot(x1-x0,dfinv))
                except Exception:
                    try:
                        dfinv=npla.inv(mtb.jacobian(f,x0,args=args,kwargs=kwargs))
                    except Exception:
                        dfinv=np.zeros((dim,dim))
        elif mode=="good_broyden":
            try:
                dfinv=dfinv+np.outer(((x1-x0)-np.dot(dfinv,f1-f0))/np.dot(x1-x0,np.dot(dfinv,f1-f0)),np.dot(x1-x0,dfinv))
            except Exception:
                try:
                    dfinv=dfinv+np.outer(((x1-x0)-np.dot(dfinv,f1-f0))/sum((f1-f0)**2),f1-f0)
                except Exception:
                    try:
                        dfinv=npla.inv(mtb.jacobian(f,x0,args=args,kwargs=kwargs))
                    except Exception:
                        dfinv=np.zeros((dim,dim))
        else:
            try:
                dfinv=mtb.jacobian(f,x1,args=args,kwargs=kwargs)
            except Exception:
                dfinv=np.zeros((dim,dim))

        if inform:
            F.append(nf1)
        min_flag=(nf1<=errtol)
        extreme_flag=(npla.norm(x1-x0)<=zerotol)
        x0=x1.copy()
        f0=f1.copy()
        nf0=nf1.copy()
        newt_count+=1
        
    if not min_flag and not extreme_flag:
        if inform:
            print("Unable to find root or critical point after %i iterations"%newt_count)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return None
    else:
        if inform:
            if min_flag:
                print("Root found after %i iterations"%newt_count)
                print("||f(x)||_2 = %0.6f"%nf1)
            else:
                print("Extremum found after %i iterations"%newt_count)
                print("||f(x)||_2 = %0.6f"%nf1)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return x1

def minimize(f,x0,args=[],kwargs={},errtol=1e-6,zerotol=1e-8,maxlevel=100,mode="bfgs",adapt=True,inform=False):
    #This method performs a BFGS approximation of the minimum of the function 
    #f:R->R, using vector x0 as it's initial guess. Variable errtol sets the
    #minimum norm that the function gradient can take on before an approximate
    #of the minimum is made. Variable maxlevel determines the maximum number of
    #iterations the method can perform before it is forced to end.
    #If variable mode is set to "newton_raphson", the inverse Hessian is
    #computed every iteration. If variable mode is set to "bfgs", the
    #inverse Hessian is approximated using the BFGS method (default).
    #If variable adapt is set to True, the method uses the weak Wolfe 
    #conditions to damp (or accelerate) the Newton-Raphson iteration to coerce
    #global converge to a minimum or minimum of the function's gradient.
    #If variable inform is set to True, the method relays information on 
    #it's converge behavior to the user. 
    
    df=lambda X:gtb.differentiate(f,X,args=args,kwargs=kwargs)
    try:
        df0=df(x0,*args,**kwargs)
    except Exception:
        print("Error: unable to initialize algorithm using inputted vector x0")
        return x0
    ndf0=abs(df0)
    
    if ("int" in type(x0).__name__) or ("float" in type(x0).__name__):
        if ("int" not in type(df0).__name__) and ("float" not in type(df0).__name__):
            if inform:
                print("Error: unable to apply Newton-Raphson iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Newton-Raphson iteration to this function")
        return None
    
    errtol=abs(errtol)
    zerotol=abs(zerotol)
    maxlevel=abs(maxlevel)
    
    mode=mode.lower()
    if ("newt" in mode) or ("raph" in mode):
        mode="newton_raphson"    
        try:
            dfinv=1.0/gtb.differentiate(df,x0,args=args,kwargs=kwargs)
        except Exception:
            try:
                dfinv=1.0/ndf0
            except Exception:
                dfinv=0.0
    else:
        dfinv=1.0/ndf0
        mode="bfgs"
    
    alpha=1.0
    min_flag=(ndf0<=errtol)
    extreme_flag=False
    newt_count=0
    if inform:
        F=[ndf0]
    while not min_flag and not extreme_flag and newt_count<maxlevel:
        delta_x=-dfinv*df0
        if adapt and delta_x*df0<-zerotol:
            if mode=="newton_raphson":
                alpha=line_search(f,x0,delta_x,args=args,kwargs=kwargs)
            else:
                alpha=line_search(f,x0,delta_x,args=args,kwargs=kwargs,wolfe_mode=False)
        x1=x0+alpha*delta_x
        df1=df(x1)
        ndf1=abs(df1)
        if mode=="newton_raphson":
            try:
                dfinv=1.0/gtb.differentiate(df,x0,args=args,kwargs=kwargs)
            except Exception:
                try:
                    dfinv=delta_x/(df1-df0)
                except Exception:
                    try:
                        dfinv=1.0/ndf1
                    except Exception:
                        dfinv=0.0
        else:
            try:
                dfinv=delta_x/(df1-df0)
            except Exception:
                try:
                    dfinv=1.0/gtb.differentiate(df,x0,args=args,kwargs=kwargs)
                except Exception:
                    try:
                        dfinv=1.0/ndf1
                    except Exception:
                        dfinv=0.0

        if inform:
            F.append(ndf1)
        min_flag=(ndf1<=errtol)
        extreme_flag=(abs(x1-x0)<=errtol)
        x0=x1
        df0=df1
        ndf0=ndf1
        newt_count+=1
        
    if not min_flag and not extreme_flag:
        if inform:
            print("Unable to find minimum after %i iterations"%newt_count)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Gradient Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Gradient Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return None
    else:
        if inform:
            if min_flag:
                print("Minimum found after %i iterations"%newt_count)
                print("|diff_f(x)| = %0.6f"%ndf0)
            else:
                print("Extremum of gradient found after %i iterations"%newt_count)
                print("|diff_f(x)| = %0.6f"%ndf0)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Gradient Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Gradient Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return x0

def multi_minimize(f,x0,args=[],kwargs={},errtol=1e-6,zerotol=1e-8,maxlevel=100,mode="bfgs",adapt=True,inform=False):
    #This method performs a BFGS approximation of the minimum of the function 
    #f:R^n->R, using vector x0 as it's initial guess. Variable errtol sets the
    #minimum norm that the function gradient can take on before an approximate
    #of the minimum is made. Variable maxlevel determines the maximum number of
    #iterations the method can perform before it is forced to end.
    #If variable mode is set to "newton_raphson", the inverse Hessian is
    #computed every iteration. If variable mode is set to "bfgs", the
    #inverse Hessian is approximated using the BFGS method (default).
    #If variable adapt is set to True, the method uses the weak Wolfe 
    #conditions to damp (or accelerate) the Newton-Raphson iteration to coerce
    #global converge to a minimum or minimum of the function's gradient.
    #If variable inform is set to True, the method relays information on 
    #it's converge behavior to the user. 
    
    x0=np.asarray(x0)
    df=lambda X:mtb.grad(f,X,args=args,kwargs=kwargs)
    try:
        df0=df(x0,*args,**kwargs)
    except Exception:
        print("Error: unable to initialize algorithm using inputted vector x0")
        return x0
    ndf0=npla.norm(df0)
    
    if ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        if ("list" in type(df0).__name__) or ("tuple" in type(df0).__name__) or ("ndarray" in type(df0).__name__):
            if len(x0)==len(df0):
                dim=len(x0)
            else:
                if inform:
                    print("Error: unable to apply Newton-Raphson iteration to f:R^%i->R^%i"%(len(x0),len(df0)))
                return None
        else:
            if inform:
                print("Error: unable to apply Newton-Raphson iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Newton-Raphson iteration to this function")
        return None
    
    errtol=abs(errtol)
    zerotol=abs(zerotol)
    maxlevel=abs(maxlevel)
    
    mode=mode.lower()
    if ("newt" in mode) or ("raph" in mode):
        mode="newton_raphson"    
        try:
            dfinv=mtb.jacobian(df,x0,args=args,kwargs=kwargs)
        except Exception:
            if abs(ndf0)>zerotol:
                dfinv=ndf0*np.identity(dim)
            else:   
                dfinv=np.zeros(dim,dim)
    else:
        dfinv=ndf0*np.identity(dim)
        if "broyden" in mode:
            mode="broyden"
        elif "dfp" in mode:
            mode="dfp"
        elif ("sr" in mode) or ("1" in mode):
            mode="sr-1"
        else:
            mode="bfgs"
    
    alpha=1.0
    min_flag=(ndf0<=errtol)
    extreme_flag=False
    newt_count=0
    if inform:
        F=[ndf0]
    while not min_flag and not extreme_flag and newt_count<maxlevel:
        if mode=="newton_raphson":
            delta_x=-npla.solve(dfinv,df0)    
        else:    
            delta_x=-np.dot(dfinv,df0)
        if adapt and np.inner(delta_x,df0)<0:
            if mode=="newton_raphson":
                alpha=multi_line_search(f,x0,delta_x,args=args,kwargs=kwargs)
            else:
                alpha=multi_line_search(f,x0,delta_x,args=args,kwargs=kwargs,wolfe_mode=False)
        x1=x0+alpha*delta_x
        df1=df(x1)
        ndf1=npla.norm(df1)
        if mode=="newton_raphson":
            try:
                dfinv=mtb.jacobian(df,x0,args=args,kwargs=kwargs)
            except Exception:
                if abs(ndf1)>zerotol:
                    dfinv=ndf1*np.identity(dim)
                else:
                    dfinv=np.zeros((dim,dim))
        else:
            try:
                delta_df=df1-df0
                if mode=="broyden":
                    res1=np.dot(dfinv,delta_df)
                    dfinv=dfinv+np.outer(delta_x-res1,np.dot(delta_x,dfinv))/np.inner(delta_x,res1)
                elif mode=="dfp":
                    res1=np.dot(dfinv,delta_df)
                    dfinv=dfinv+np.outer(delta_x,delta_x)/np.inner(delta_x,delta_df)-np.outer(res1,res1)/np.inner(delta_df,res1)
                elif mode=="sr-1":
                    res1=delta_x-np.dot(dfinv,delta_df)
                    dfinv=dfinv+np.outer(res1,res1)/np.inner(res1,delta_df)
                else:
                    res1=np.inner(delta_df,delta_x)
                    res2=np.dot(dfinv,np.outer(delta_df,delta_x))
                    dfinv=dfinv+(np.inner(delta_df,delta_x)+np.inner(delta_df,np.dot(dfinv,delta_df)))*np.outer(delta_x,delta_x)/res1**2-(res2+np.transpose(res2))/res1
            except Exception:
                try:
                    dfinv=npla.inv(mtb.jacobian(df,x0,args=args,kwargs=kwargs))
                except Exception:
                    try:
                        dfinv=np.identity(dim)/ndf1
                    except Exception:
                        dfinv=np.zeros((dim,dim))

        if inform:
            F.append(ndf1)
        min_flag=(ndf1<=errtol)
        extreme_flag=(npla.norm(x1-x0)<=errtol)
        x0=x1.copy()
        df0=df1.copy()
        ndf0=ndf1
        newt_count+=1
        
    if not min_flag and not extreme_flag:
        if inform:
            print("Unable to find minimum after %i iterations"%newt_count)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Gradient Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Gradient Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return None
    else:
        if inform:
            if min_flag:
                print("Minimum found after %i iterations"%newt_count)
                print("||grad_f(x)||_2 = %0.6f"%ndf0)
            else:
                print("Extremum of gradient found after %i iterations"%newt_count)
                print("||grad_f(x)||_2 = %0.6f"%ndf0)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Gradient Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Gradient Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return x0

def sqp(f,x0,h=lambda X:[],g=lambda X:[],errtol=1e-6,zerotol=1e-8,maxlevel=100,inform=False,mode="broyden"):
    if ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        x0=np.asarray(x0)
        n=len(x0)
        try:
            if ("int" not in type(f(x0)).__name__) and ("float" not in type(f(x0)).__name__):
                if inform:
                    print("Error: objective function f is not a R^%i->R function"%n)
                return None
        except Exception:
            if inform:
                print("Error: unable to initialize algorithm by applying objective function f to point x0")
            return None
        
        try:
            if ("list" in type(h(x0)).__name__) or ("tuple" in type(h(x0)).__name__) or ("ndarray" in type(h(x0)).__name__):
                m_e=len(h(x0))
            else:
                if inform:
                    print("Error: equality constraint function h is not a R^%i->R^m_e function"%n)
                return None
        except Exception:
            if inform:
                print("Error: unable to initialize algorithm by applying equality constraint function h to point x0")
            return None
        
        try:
            if ("list" in type(g(x0)).__name__) or ("tuple" in type(g(x0)).__name__) or ("ndarray" in type(g(x0)).__name__):
                m_i=len(g(x0))
            else:
                if inform:
                    print("Error: inequality constraint function g is not a R^%i->R^m_i function"%n)
                return None
        except Exception:
            if inform:
                print("Error: unable to initialize algorithm by applying inequality constraint function g to point x0")
            return None
    else:
        if inform:
            print("Error: unable to initialize SQP algorithm with inputted point x0.")
        return None
    
    mode=mode.lower()
    if "b" in mode and "f" in mode and "g" in mode:
        mode="bfgs"
    elif "b" in mode:
        mode="broyden"
    elif "d" in mode and "f" in mode and "p" in mode:
        mode="dfp"
    elif "s" in mode and "r" in mode and "1" in mode:
        mode="sr1"
    else:
        mode="newton_raphson"
    
    errtol=abs(errtol)
    zerotol=abs(zerotol)
    maxlevel=abs(maxlevel)
    
    if m_i>0:
        res_G=g(x0)
        A=[i for i in range(m_i) if res_G[i]>=-abs(zerotol)]
        m_A=len(A)
        I=[i for i in range(m_i) if res_G[i]<-abs(zerotol)]
        m_I=len(I)
    else:
        A=[]
        m_A=0
        I=[]
        m_I=0
    
    def g_A(x):
        res_G=g(x)
        return np.array([res_G[i] for i in A])
    
    def H(x):
        try:
            B=np.zeros((n+m_e+m_A,n+m_e+m_A))
            B[:n,:n]=mtb.jacobian(lambda X:mtb.grad(f,X),x)
            if m_e>0:
                B[n:n+m_e,:n]=mtb.jacobian(h,x)
                B[:n,n:n+m_e]=B[n:n+m_e,:n].T
            if m_A>0:
                B[n+m_e:,:n]=mtb.jacobian(g_A,x)
                B[:n,n+m_e:]=B[n+m_e:,:n].T
            return B
        except Exception:
            return np.identity(n+m_e+m_A)
    
    def delta_f(x):
        delta_f=np.zeros(n+m_e+m_A)
        delta_f[:n]=mtb.grad(f,x)
        if m_e>0:
            delta_f[n:n+m_e]=h(x)
        if m_A>0:
            delta_f[n+m_e:]=g_A(x)
        return delta_f
        
    D_F=delta_f(x0)
    if mode=="newton_raphson":
        B=H(x0)
    else:
        try:
            B=npla.inv(H(x0))
        except npla.LinAlgError as err:
            if "Singular matrix" in str(err):
                B=np.identity(n+m_e+m_A)/npla.norm(D_F)
            else:
                raise
    if inform:
        F=[f(x0)]
    sqp_bool=True
    count=0
    while sqp_bool and count<maxlevel:
        if mode=="newton_raphson":
            try:
                res_d=-npla.solve(B,D_F)
            except npla.LinAlgError as err:
                if "Singular matrix" in str(err):
                    res_d=-1.0*D_F/npla.norm(D_F)
                else:
                    raise
        else:
            res_d=-np.dot(B,D_F)
        delta_x=np.zeros(n+m_e+m_i)
        delta_x[:n+m_e]=res_d[:n+m_e]
        for i in range(m_A):
            delta_x[n+m_e+A[i]]+=res_d[n+m_e+i]
        
        if npla.norm(delta_x[:n])<=errtol:
            sqp_bool=False
            if m_A>0:
                for i in range(m_A):
                    if delta_x[n+m_e+A[i]]<-abs(zerotol):
                        I.append(A[i])
                        m_I+=1
                        A=A[:i]+A[i+1:]
                        m_A-=1
                        sqp_bool=True
                        D_F=delta_f(x0)
                        if mode=="newton_raphson":
                            B=H(x0)
                        else:
                            try:
                                B=npla.inv(H(x0))
                            except npla.LinAlgError as err:
                                if "Singular matrix" in str(err):
                                    B=np.identity(n+m_e+m_A)/npla.norm(D_F)
                                else:
                                    raise
                        break
        else:
            alpha=1.0
            if m_I>0:
                res_G=g(x0)
                res_JG=mtb.jacobian(g,x0)
                for i in I:
                    res1=res_G[i]
                    res2=np.inner(res_JG[i,:],delta_x[:n])
                    if abs(res2)>abs(zerotol):
                        res=-res1/res2
                    else:
                        res=1.0
                    if 0<res<=alpha:
                        alpha=res
            x0=x0+alpha*delta_x[:n]
            changed_bool=True
            if m_i>0:
                res_G=g(x0)
                temp_A=A.copy()
                temp_m_A=m_A
                A=[i for i in range(m_i) if res_G[i]>=-abs(zerotol)]
                m_A=len(A)
                if temp_m_A!=m_A or temp_A!=A:
                    changed_bool=False
                I=[i for i in range(m_i) if res_G[i]<-abs(zerotol)]
                m_I=len(I)
            if mode=="newton_raphson":
                B=H(x0)
                D_F=delta_f(x0)
            else:
                if changed_bool:
                    if mode=="bfgs":
                        try:
                            res_d=alpha*res_d
                            temp_D_F=D_F.copy()
                            D_F=delta_f(x0)
                            res1=D_F-temp_D_F
                            res2=np.inner(res_d,res1)
                            res3=np.dot(B,np.outer(res1,res_d))
                            B=B+(res2+np.inner(res1,np.dot(B,res1)))*np.outer(res_d,res_d)/(res2**2)-(res3+res3.T)/res2
                        except Exception:
                            D_F=delta_f(x0)
                            try:
                                B=npla.inv(H(x0))
                            except npla.LinAlgError as err:
                                if "Singular matrix" in str(err):
                                    B=np.identity(n+m_e+m_A)/npla.norm(D_F)
                                else:
                                    raise
                    elif mode=="broyden":
                        try:
                            res_d=alpha*res_d
                            temp_D_F=D_F.copy()
                            D_F=delta_f(x0)
                            res1=D_F-temp_D_F
                            res2=np.dot(B,res1)
                            B=B+np.dot(np.outer((res-res2),res),B)/np.inner(res,res2)
                        except Exception:
                            D_F=delta_f(x0)
                            try:
                                B=npla.inv(H(x0))
                            except npla.LinAlgError as err:
                                if "Singular matrix" in str(err):
                                    B=np.identity(n+m_e+m_A)/npla.norm(D_F)
                                else:
                                    raise
                    elif mode=="dfp":
                        try:
                            res_d=alpha*res_d
                            temp_D_F=D_F.copy()
                            D_F=delta_f(x0)
                            res1=D_F-temp_D_F
                            res2=np.dot(B,res)
                            B=B+np.outer(res,res)/np.inner(res,res1)-np.outer(res2,res2)/np.inner(res1,np.dot(B,res1))
                        except Exception:
                            D_F=delta_f(x0)
                            try:
                                B=npla.inv(H(x0))
                            except npla.LinAlgError as err:
                                if "Singular matrix" in str(err):
                                    B=np.identity(n+m_e+m_A)/npla.norm(D_F)
                                else:
                                    raise
                    elif mode=="sr1":
                        try:
                            res_d=alpha*res_d
                            temp_D_F=D_F.copy()
                            D_F=delta_f(x0)
                            res1=D_F-temp_D_F
                            res2=res-np.dot(B,res)
                            B=B+np.outer(res2,res)/np.inner(res2,res)
                        except Exception:
                            D_F=delta_f(x0)
                            try:
                                B=npla.inv(H(x0))
                            except npla.LinAlgError as err:
                                if "Singular matrix" in str(err):
                                    B=np.identity(n+m_e+m_A)/npla.norm(D_F)
                                else:
                                    raise
                    else:
                        D_F=delta_f(x0)
                        try:
                            B=npla.inv(H(x0))
                        except npla.LinAlgError as err:
                            if "Singular matrix" in str(err):
                                B=np.identity(n+m_e+m_A)/npla.norm(D_F)
                            else:
                                raise
                else:
                    D_F=delta_f(x0)
                    try:
                        B=npla.inv(H(x0))
                    except npla.LinAlgError as err:
                        if "Singular matrix" in str(err):
                            B=np.identity(n+m_e+m_A)/npla.norm(D_F)
                        else:
                            raise
        if inform:
            F.append(f(x0))
        count+=1
        
    if npla.norm(delta_x[:n])<=errtol and all(delta_x[n+m_e:]>=-abs(zerotol)):
        if inform:
            print("Minimum found after %i iterations using %s"%(count,mode))
            print("f(x) = %0.6f"%f(x0))
            ax1=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax1.plot([i for i in range(len(F))],F)
            ax1.set_title("Function Value per Iteration (%s)"%mode,fontdict=font)
            ax1.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax1.set_ylabel("Function Value",fontsize=16)
            ax1.xaxis.set_tick_params(labelsize=16)
            ax1.yaxis.set_tick_params(labelsize=16)
        return x0
    else:
        if inform:
            if count==maxlevel:
                print("Unable to find minimum after %i iterations using %s"%(count,mode))
            else:
                print("Converged to suboptimal point after %i iterations using %s"%(count,mode))
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Value per Iteration (%s)"%mode,fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Value",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return None
    
def simulated_annealing(f,x0,T=100,alpha_T=0.95,inform=False,adapt=False):
    if ("int" in type(x0).__name__) or ("float" in type(x0).__name__):
        n=0
        try:
            f0=f(x0)
            if ("int" not in type(f0).__name__) and ("float" not in type(f0).__name__):
                if inform:
                    print("Error: objective function f is not a R^%i->R function"%n)
                return None
        except Exception:
            if inform:
                print("Error: unable to initialize algorithm by applying objective function f to point x0")
            return None
    
    elif ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        x0=np.asarray(x0)
        n=len(x0)
        try:
            f0=f(x0)
            if ("int" not in type(f0).__name__) and ("float" not in type(f0).__name__):
                if inform:
                    print("Error: objective function f is not a R^%i->R function"%n)
                return None
        except Exception:
            if inform:
                print("Error: unable to initialize algorithm by applying objective function f to point x0")
            return None
    else:
        if inform:
            print("Error: unable to initialize SQP algorithm with inputted point x0.")
        return None
    
    if alpha_T>=1.0 or alpha_T<=0:
        alpha_T=0.95
    T=max(T,1.0)
    
    if inform:
        F=[]
    X=[]
    count=0
    maxlevel=(np.log(1e-6)-np.log(T))/np.log(alpha_T)
    if n==0:
        while count<maxlevel:
            delta_x=np.random.normal(loc=0.0,scale=1.0)
            if adapt:
                alpha_delta_x=line_search(f,x0,delta_x)
                x1=x0+alpha_delta_x*delta_x
            else:
                x1=x0+delta_x
            f1=f(x1)
            res_f=f0-f1
            if res_f>=0:
                x0=x1
                f0=f1
                X.append(x0)
            elif np.random.rand()<=np.exp(res_f/T):
                x0=x1
                f0=f1
                X.append(x0)
            T=alpha_T*T
            count+=1
            if inform:
                F.append(f0)
    else:
        while count<maxlevel:
            delta_x=np.random.normal(loc=0.0,scale=1.0,size=n)
            if adapt:
                alpha_delta_x=multi_line_search(f,x0,delta_x)
                x1=x0+alpha_delta_x*delta_x
            else:
                x1=x0+delta_x
            f1=f(x1)
            res_f=f0-f1
            if res_f>=0:
                x0=x1.copy()
                f0=f1
            elif np.random.rand()<=np.exp(res_f/T):
                x0=x1.copy()
                f0=f1
            T=alpha_T*T
            count+=1
            if inform:
                F.append(f0)
            
    if inform:
        print("Heuristic minimum found after %i iterations"%count)
        print("f(x) = %0.6f"%f0)
        ax1=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
        ax1.plot([i for i in range(len(F))],F)
        ax1.set_title("Function Value per Iteration",fontdict=font)
        ax1.set_xlabel("Iteration",fontsize=16,rotation=0)
        ax1.set_ylabel("Function Value",fontsize=16)
        ax1.xaxis.set_tick_params(labelsize=16)
        ax1.yaxis.set_tick_params(labelsize=16)
    return x0
