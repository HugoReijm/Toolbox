import numpy as np
import numpy.linalg as npla
import toolbox.generaltoolbox as gtb
import toolbox.matrixtoolbox as mtb
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("error")

graphsize=9
font = {"family": "serif",
    "color": "black",
    "weight": "bold",
    "size": "20"}

def simplex(A,b,c,inequalities=None,mode="maximize",inform=False):
    #This method solves the optimization problem
    #"maximize c^Tx subject to Ax<=b and x>=0" using the Simplex Method,
    #assuming the set of feasible solutions is convex. Variable <inform> allows
    #for the user to see diagnostic data of the method.
    
    if not isinstance(A,np.ndarray):
        try:
            A=np.array(A)
        except Exception:
            print("Error: Matrix A of incompatible type.")
            return None
    if not isinstance(b,np.ndarray):
        try:
            b=np.array(b)
        except Exception:
            print("Error: Vector b of incompatible type")
            return None
    if not isinstance(c,np.ndarray):
        try:
            c=np.array(c)
        except Exception:
            print("Vector c of incompatible type")
            return None
    if A.shape[1]!=c.shape[0] or A.shape[0]!=b.shape[0]:
        print("A,b, and c of incompatible sizes")
        return None
    
    n=c.shape[0]
    m=0
    if isinstance(inequalities,(list,tuple,np.ndarray)):
        inequalities=np.array(inequalities)
        for i in range(min(A.shape[0],inequalities.shape[0])):
            try:
                inequalities[i]=inequalities[i].lower()
                if ">" in inequalities[i] or "great" in inequalities[i]:
                    inequalities[i]=">"
                    m+=1
                #elif "=" in inequalities[i] or "equal" in inequalities[i]:
                #    inequalities[i]="="
                    #SLIGHT ERROR: MUST FIRST BE ON A FEASIBLE SOLUTION
                else:
                    inequalities[i]="<"
                    m+=1
            except Exception:
                inequalities[i]="<"
                m+=1
        m+=A.shape[0]-inequalities.shape[0]
        inequalities=np.hstack((inequalities,["<" for i in range(inequalities.shape[0],A.shape[0])]))
    else:
        m=A.shape[0]
        inequalities=["<" for i in range(A.shape[0])]
    
    d=np.hstack((c,np.zeros(m)))
    if isinstance(mode,str):
        if "min" in mode.lower():
            r1=np.hstack((d,0))
        else:
            r1=np.hstack((-d,0))
    else:
        r1=np.hstack((-d,0))
    
    
    A=np.hstack((A,np.zeros((A.shape[0],m))))
    col_count=n
    for i in range(A.shape[0]):
        if inequalities[i]==">":
            A[i,col_count]=-1.0
            col_count+=1
        elif inequalities[i]=="<":
            A[i,col_count]=1.0
            col_count+=1
    r2=np.hstack((A,np.array([b]).T))
    S=np.vstack((r1,r2))
    
    iter_count=0
    pivot=True
    while pivot:
        pivot_col=-1
        max_cost=0
        for j in range(n+m):
            if S[0,j]<max_cost:
                pivot_col=j
                max_cost=S[0,j]
                #break
        if pivot_col==-1:
            pivot=False
            break
        
        pivot_row=-1
        min_ratio=np.inf
        for i in range(1,S.shape[0]):
            if S[i,pivot_col]!=0:
                res=S[i,-1]/S[i,pivot_col]
                if 0<res<min_ratio:
                    pivot_row=i
                    min_ratio=res
        if pivot_row==-1:
            pivot=False
            break
        
        if S[pivot_row,pivot_col]!=1.0:
            S[pivot_row,:]=S[pivot_row,:]/S[pivot_row,pivot_col]
            S[pivot_row,pivot_col]=1.0
        
        for i in range(S.shape[0]):
            if i!=pivot_row and S[i,pivot_col]!=0:
                S[i,:]=S[i,:]-S[pivot_row,:]*S[i,pivot_col]
                S[i,pivot_col]=0.0
        
        iter_count+=1
    
    sol=np.zeros(S.shape[1]-1)
    for j in range(n+m):
        basis_bool=True
        one_pos=-1
        for i in range(1,S.shape[0]):
            if S[i,j]!=0.0:
                if S[i,j]!=1.0:
                    basis_bool=False
                    break
                else:
                    if one_pos!=-1:
                        basis_bool=False
                        break
                    else:
                        one_pos=i
        if basis_bool and one_pos!=-1:
            sol[j]=S[one_pos,-1]
    
    if inform:
        print("Optimal Variable Values:")
        for j in range(n):
            if sol[j]!=0.0:
                print("x_%i = %0.6f"%(j+1,sol[j]))
        print("Optimal Slack Variable Values:")
        for i in range(n,n+m):
            if sol[i]!=0.0:
                print("x_%i = %0.6f"%(i+1,sol[i]))
        print("Maximal Value: %0.6f"%S[0,-1])
        print("Number of Iterations: %i"%iter_count)

    return sol[:n]

def nelder_mead(f,x0,args=[],kwargs={},errtol=1e-6,maxlevel=100,inform=False):
    class simplex(object):
        def __init__(self,V,f):
            self.V=V.copy()
            self.f=f
            self.F=np.array([self.f(v,*args,**kwargs) for v in V])
            
        def order(self):
            index=np.argsort(self.F)
            self.V=self.V[index]
            self.F=self.F[index]
            self.C=np.mean(self.V[:-1],axis=0)
            self.FC=self.f(self.C,*args,**kwargs)
            
        def reflect(self,a=1.0):
            v=self.C+a*(self.C-self.V[-1])
            fv=self.f(v,*args,**kwargs)
            if fv<=self.F[0]:
                self.expand(v,fv)
            elif fv<=self.F[-2]:
                self.V[-1]=v.copy()
                self.F[-1]=fv
            elif fv<self.F[-1]:
                self.contract(v,fv)
            else:
                self.contract(v,fv,inner=True)
        
        def expand(self,vr,fvr,a=2.0):
            v=self.C+a*(self.C-self.V[-1])
            fv=self.f(v,*args,**kwargs)
            if fv<=self.F[0]:
                #self.expand(v,fv,a=2*a)
                self.V[-1]=v.copy()
                self.F[-1]=fv
            else:
                self.V[-1]=vr.copy()
                self.F[-1]=fvr
        
        def contract(self,vr,fvr,a=0.5,inner=False):
            if inner:
                v=self.C-a*(self.C-self.V[-1])
            else:
                v=self.C+a*(self.C-self.V[-1])
            fv=self.f(v,*args,**kwargs)
            if (inner and fv<=self.F[-1]) or (not inner and fv<=fvr):
                self.V[-1]=v.copy()
                self.F[-1]=fv
            else:
                self.shrink()
        
        def shrink(self,a=0.5):
            self.V[1:]=self.V[1:]-a*(self.V[1:]-self.V[0])
            self.F[1:]=[self.f(v,*args,**kwargs) for v in V[1:]]
    
    if ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        x0=np.asarray(x0)
        dim=len(x0)
        try:
            f0=f(x0,*args,**kwargs)
            if ("int" not in type(f0).__name__) and ("float" not in type(f0).__name__):
                if inform:
                    print("Error: unable to apply Nelder-Mead iteration to this function")
                return None
        except Exception:
            if inform:
                print("Error: unable to apply Nelder-Mead iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Nelder-Mead iteration to this function")
        return None
    
    V=[x0]
    for i in range(dim):
        res=np.zeros(dim)
        res[i]=1.0
        V.append(x0+res)
    V=np.array(V)
    s=simplex(V,f)
    std=np.std(s.F)
    count=0
    if inform:
        plotline=[f(np.mean(s.V,axis=0),*args,**kwargs)]
    while std>errtol and count<maxlevel:
        s.order()
        s.reflect()
        std=np.std(s.F)
        if inform:
            plotline.append(s.F[0])
        count+=1
    
    if inform:
        print("Solution after %i iterations"%count)
        print("F(x)=%0.6f"%s.F[0])
        ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
        ax.plot([i for i in range(len(plotline))],plotline)
        ax.set_title("Simplex Min Function Value per Iteration",fontdict=font)
        ax.set_xlabel("Iteration",fontsize=16,rotation=0)
        ax.set_ylabel("Function Value",fontsize=16)
        ax.xaxis.set_tick_params(labelsize=16)
        ax.yaxis.set_tick_params(labelsize=16)
    return s.V[0]
    
def exact_line_search(f,start,stop,args=[],kwargs={},errtol=1e-6,maxlevel=100,inform=False):
    #This method performs a line search of a function f:R->R to find the
    #minimum of f between variables start and stop using the Brent minimization
    #method, which itself is a hybrid of Golden Section Search and Jarrat
    #iteration. It is guaranteed to converge, and usually does super-linearly.
    #Variable errtol sets the minimum distance between start and stop before an
    #approximate of the minimum is made. Variable maxlevel determines the
    #maximum number of iterations the method can perform before it is forced to
    #end. If variable inform is set to true, the method relays information on 
    #it's converge behavior to the user. 
    if stop<start:
        temp=start
        start=stop
        stop=temp
    phi=(1+np.sqrt(5))/2
    x=stop-(phi-1)*(stop-start)
    fx=f(x,*args,**kwargs)
    v=x
    w=x
    fv=fx
    fw=fx
    d=0
    e=0
    m=(start+stop)/2
    count=1
    if inform:
        F=[f(m,*args,**kwargs)]
    while stop-start>errtol and count<maxlevel:
        r=(x-w)*(fx-fv)
        tq=(x-v)*(fx-fw)
        tp=(x-v)*tq-(x-w)*r
        tq2=2*(tq-r)
        if tq2>0:
            p=-tp
            q=tq2
        else:
            p=tp
            q=-tq2
        safe=(q!=0.0)
        if safe:
            try:
                deltax=p/q
            except Exception:
                deltax=0.0
        else:
            deltax=0.0
        parabolic=(safe and (start<x+deltax<stop) and (abs(deltax)<abs(e)/2))
        if parabolic:
            e=d
            d=deltax
        else:
            if x<m:
                e=stop-x
            else:
                e=start-x
            d=(2-phi)*e
        u=x+d
        fu=f(u,*args,**kwargs)
        if fu<=fx:
            if u<x:
                stop=x
            else:
                start=x
            v=w
            w=x
            x=u
            fv=fw
            fw=fx
            fx=fu
        else:
            if u<x:
                start=u
            else:
                stop=u
            if fu<=fw or w==x:
                v=w
                w=u
                fv=fw
                fw=fu
            elif fu<=fv or v==x or v==w:
                v=u
                fv=fu
        m=(start+stop)/2
        if inform:
            F.append(f(m,*args,**kwargs))
        count+=1
    if stop-start>errtol:
        if inform:
            print("Unable to find minimum after %i iterations"%count)
        return None
    else:    
        if inform:
            print("Minimum found after %i iterations"%count)
            print("f(x) = %0.6f"%F[-1])
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Value per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Value",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return m

def line_search(f,x0,delta_x,args=[],kwargs={},c1=1e-4,c2=0.9,mode="Wolfe"):
    mode=mode.lower()
    if "wolf" in mode:
        mode="wolfe"
    else:
        mode="goldstein"
    f0=f(x0)
    a=0.0
    alpha=1.0
    b=np.inf
    bisect_count=0
    cond_flag=False
    if ("int" in type(x0).__name__) or ("float" in type(x0).__name__):
        grad_f0=gtb.differentiate(f,x0,args=args,kwargs=kwargs)
        while not cond_flag and bisect_count<100:
            try:
                res1=f(x0+alpha*delta_x)
                if res1>f0+c1*alpha*delta_x*grad_f0:
                    b=alpha
                    res2=gtb.grad(f,x0,args=args,kwargs=kwargs)
                    alpha=-np.inner(res2,delta_x)*b**2/(res1-f0-np.inner(res2,delta_x)*b)
                    #alpha=0.5*(a+b)
                else:
                    res2=gtb.differentiate(f,x0+alpha*delta_x,args=args,kwargs=kwargs)
                    if ((mode=="wolfe" and res2<c2*grad_f0)
                            or (mode=="goldstein" and res1<f0+(1-c1)*alpha*delta_x*grad_f0)):
                        a=alpha
                        if b<np.inf:
                            alpha=0.5*(a+b)
                        else:
                            alpha=2*a
                    else:
                        cond_flag=True
            except Exception:
                b=alpha
                alpha=0.5*(a+b)
            bisect_count+=1
    elif ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        grad_f0=mtb.grad(f,x0,args=args,kwargs=kwargs)
        while not cond_flag and bisect_count<100:
            try:
                res1=f(x0+alpha*delta_x)
                res2=mtb.grad(f,x0+alpha*delta_x,args=args,kwargs=kwargs)
                if ((mode=="wolfe" and res1>f0+c1*alpha*np.dot(delta_x,grad_f0))
                        or (mode=="goldstein" and res1>f0+c1*alpha*np.dot(grad_f0,delta_x))):
                    b=alpha
                    alpha=0.5*(a+b)
                elif ((mode=="wolfe" and np.dot(delta_x,res2)<c2*np.dot(delta_x,grad_f0))
                        or (mode=="goldstein" and res1<f0+(1-c1)*alpha*np.dot(delta_x,grad_f0))):
                    a=alpha
                    if b<np.inf:
                        alpha=0.5*(a+b)
                    else:
                        alpha=2*a
                else:
                    cond_flag=True
            except Exception:
                b=alpha
                alpha=0.5*(a+b)
            bisect_count+=1
    else:
        return None
    return alpha

def newton_Raphson(f,x0,args=[],kwargs={},errtol=1e-6,maxlevel=100,mode="bad_broyden",adapt=True,inform=False):
    #This method performs a Newton-Raphson approximation of the root of the 
    #function f:R->R, using scalar x0 as it's initial guess.
    #Variable errtol sets the minimum norm that the function can take on before
    #an approximate of the root is made. Variable maxlevel determines the
    #maximum number of iterations the method can perform before it is forced to
    #end. If variable mode is set to newton_raphson, the inverse Jacobian is
    #computed every iteration. If variable mode is set to broyden (default),
    #the inverse Jacobian is approximated using the Broyden method. If variable
    #adapt is set to True, the method uses the weak Wolfe conditions to damp
    #(or accelerate) the Newton-Raphson iteration to coerce global converge to
    #a minimum or root. If variable inform is set to true, the method relays
    #information on it's converge behavior to the user. 
    
    f0=f(x0,*args,**kwargs)
    nf0=abs(f0)
    if ("int" in type(x0).__name__) or ("float" in type(x0).__name__):
        if ("int" not in type(f0).__name__) and ("float" not in type(f0).__name__):
            if inform:
                print("Error: unable to apply Newton-Raphson iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Newton-Raphson iteration to this function")
        return None
        
    from toolbox.generaltoolbox import differentiate
    
    c1=1e-4
    c2=0.9
    try:
        dfinv=1/differentiate(f,x0,args=args,kwargs=kwargs)
    except Exception:
        dfinv=0
    x1=x0-dfinv*f0
    f1=f(x1,*args,**kwargs)
    nf1=abs(f1)
    alpha=1.0
    
    mode=mode.lower()
    if "broyden" in mode:
        mode="broyden"
    else:
        mode="newton_raphson"
    
    newt_count=1
    if inform:
        F=[nf0,nf1]
    
    while nf1>errtol and abs(x1-x0)>errtol and newt_count<maxlevel:
        if "broyden" in mode:
            try:
                dfinv=(x1-x0)/(f1-f0)
            except Exception:
                try:
                    dfinv=1/differentiate(f,x1,args=args,kwargs=kwargs)
                except Exception:
                    dfinv=0
        else:
            try:
                dfinv=1/differentiate(f,x1,args=args,kwargs=kwargs)
            except Exception:
                try:
                    dfinv=(x1-x0)/(f1-f0)
                except Exception:
                    dfinv=0
        delta_x=-dfinv*f1
        x0=x1
        f0=f1
        nf0=nf1
        
        x1=x1+alpha*delta_x
        f1=f(x1,*args,**kwargs)
        nf1=abs(f1)
        if adapt and abs(delta_x)>0:
            nf=lambda x:abs(f(x,*args,**kwargs))
            grad_nf0=differentiate(nf,x0)
            a=0
            b=np.inf
            bisect_count=0
            wolfe_flag=False
            double_flag=True
            while not wolfe_flag and bisect_count<100:
                if nf(x0+alpha*delta_x)>nf0+c1*alpha*delta_x*grad_nf0:
                    b=alpha
                    alpha=0.5*(a+b)
                    double_flag=False
                elif delta_x*differentiate(nf,x0+alpha*delta_x)<c2*delta_x*grad_nf0:
                    a=alpha
                    if b<np.inf:
                        alpha=0.5*(a+b)
                    else:
                        alpha=2*a
                    double_flag=False
                else:
                    if double_flag and alpha<=1:
                        alpha*=2
                    else:
                        wolfe_flag=True
                bisect_count+=1
            x1=x0+alpha*delta_x
            f1=f(x1,*args,**kwargs)
            nf1=abs(f1)
        if inform:
            F.append(nf1)
        newt_count+=1

    if nf1>errtol and abs(x1-x0)>errtol:
        if inform:
            print("Unable to find extremum or minimum after %i iterations"%newt_count)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return None
    else:
        if inform:
            if nf1<=errtol:
                print("Root found after %i iterations"%newt_count)
                print("|f(x)| = %0.6f"%nf1)
            else:
                print("Minumum found after %i iterations"%newt_count)
                print("|f(x)| = %0.6f"%nf1)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return x1

def multi_Newton_Raphson(f,x0,args=[],kwargs={},errtol=1e-6,maxlevel=100,mode="bad_broyden",adapt=True,inform=False):
    #This method performs a Newton-Raphson approximation of the root of the 
    #function f:R^n->R^n, using vector x0 as it's initial guess.
    #Variable errtol sets the minimum norm that the function can take on before
    #an approximate of the root is made. Variable maxlevel determines the
    #maximum number of iterations the method can perform before it is forced to
    #end. If variable mode is set to newton_raphson, the inverse Jacobian is
    #computed every iteration. If variable mode is set to good_broyden, the
    #inverse Jacobian is approximated using the good Broyden method. If
    #variable mode is set to bad_broyden (default), the inverse Jacobian is 
    #approximated using the bad Broyden method. If variable adapt is set to
    #True, the method uses the weak Wolfe conditions to damp (or accelerate)
    #the Newton-Raphson iteration to coerce global converge to a minimum or root.
    #If variable inform is set to true, the method relays information on 
    #it's converge behavior to the user. 
    
    x0=np.asarray(x0)
    f0=f(x0,*args,**kwargs)
    nf0=npla.norm(f0)
    
    if ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        if ("list" in type(f0).__name__) or ("tuple" in type(f0).__name__) or ("ndarray" in type(f0).__name__):
            if len(x0)==len(f0):
                dim=len(x0)
            else:
                if inform:
                    print("Error: unable to apply Newton-Raphson iteration to f:R^%i->R^%i"%(len(x0),len(f0)))
                return None
        else:
            if inform:
                print("Error: unable to apply Newton-Raphson iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Newton-Raphson iteration to this function")
        return None
    
    from toolbox.matrixtoolbox import jacobian,grad
    
    c1=1e-4
    c2=0.9
    try:
        dfinv=npla.inv(jacobian(f,x0,args=args,kwargs=kwargs))
    except Exception:
        dfinv=np.zeros(dim,dim)
    delta_x=-np.dot(dfinv,f0)
    x1=x0+delta_x
    f1=f(x1,*args,**kwargs)
    nf1=npla.norm(f1)
    alpha=1.0
    
    if adapt and npla.norm(delta_x)>0:
        nf=lambda x:npla.norm(f(x,*args,**kwargs))
        grad_nf0=grad(nf,x0)
        a=0
        b=np.inf
        bisect_count=0
        wolfe_flag=False
        double_flag=True
        while not wolfe_flag and bisect_count<100:
            if nf(x0+alpha*delta_x)>nf0+c1*alpha*np.dot(delta_x,grad_nf0):
                b=alpha
                alpha=0.5*(a+b)
                double_flag=False
            elif np.dot(delta_x,grad(nf,x0+alpha*delta_x))<c2*np.dot(delta_x,grad_nf0):
                a=alpha
                if b<np.inf:
                    alpha=0.5*(a+b)
                else:
                    alpha=2*a
                double_flag=False
            else:
                if double_flag and alpha<=1:
                    alpha*=2
                else:
                    wolfe_flag=True
            bisect_count+=1
        x1=x0+alpha*delta_x
        f1=f(x1,*args,**kwargs)
        nf1=npla.norm(f1)
    
    mode=mode.lower()
    if "broyden" in mode:
        if "g" in mode:
            mode="good_broyden"
        else:
            mode="bad_broyden"
    else:
        mode="newton_raphson"
    
    newt_count=1
    if inform:
        F=[nf0,nf1]
    
    while nf1>errtol and npla.norm(x1-x0)>errtol and newt_count<maxlevel:
        if mode=="bad_broyden":
            try:
                dfinv=dfinv+np.outer(((x1-x0)-np.dot(dfinv,f1-f0))/sum((f1-f0)**2),f1-f0)
            except Exception:
                try:
                    dfinv=dfinv+np.outer(((x1-x0)-np.dot(dfinv,f1-f0))/np.dot(x1-x0,np.dot(dfinv,f1-f0)),np.dot(x1-x0,dfinv))
                except Exception:
                    try:
                        dfinv=npla.inv(jacobian(f,x0,args=args,kwargs=kwargs))
                    except Exception:
                        dfinv=np.zeros((dim,dim))
        elif mode=="good_broyden":
            try:
                dfinv=dfinv+np.outer(((x1-x0)-np.dot(dfinv,f1-f0))/np.dot(x1-x0,np.dot(dfinv,f1-f0)),np.dot(x1-x0,dfinv))
            except Exception:
                try:
                    dfinv=dfinv+np.outer(((x1-x0)-np.dot(dfinv,f1-f0))/sum((f1-f0)**2),f1-f0)
                except Exception:
                    try:
                        dfinv=npla.inv(jacobian(f,x0,args=args,kwargs=kwargs))
                    except Exception:
                        dfinv=np.zeros((dim,dim))
        else:
            try:
                dfinv=npla.inv(jacobian(f,x0,args=args,kwargs=kwargs))
            except Exception:
                try:
                    dfinv=dfinv+np.outer(((x1-x0)-np.dot(dfinv,f1-f0))/sum((f1-f0)**2),f1-f0)
                except Exception:
                    try:
                        dfinv=dfinv+np.outer(((x1-x0)-np.dot(dfinv,f1-f0))/np.dot(x1-x0,np.dot(dfinv,f1-f0)),np.dot(x1-x0,dfinv))
                    except Exception:
                        dfinv=np.zeros((dim,dim))

        delta_x=-np.dot(dfinv,f1)
        x0=x1.copy()
        f0=f1.copy()
        nf0=nf1.copy()
        x1=x1+alpha*delta_x
        f1=f(x1,*args,**kwargs)
        nf1=npla.norm(f1)
        if adapt and npla.norm(delta_x)>0:
            nf=lambda x:npla.norm(f(x,*args,**kwargs))
            grad_nf0=grad(nf,x0)
            a=0
            b=np.inf
            bisect_count=0
            wolfe_flag=False
            double_flag=True
            while not wolfe_flag and bisect_count<100:
                if nf(x0+alpha*delta_x)>nf0+c1*alpha*np.dot(delta_x,grad_nf0):
                    b=alpha
                    alpha=0.5*(a+b)
                    double_flag=False
                elif np.dot(delta_x,grad(nf,x0+alpha*delta_x))<c2*np.dot(delta_x,grad_nf0):
                    a=alpha
                    if b<np.inf:
                        alpha=0.5*(a+b)
                    else:
                        alpha=2*a
                    double_flag=False
                else:
                    if double_flag and alpha<=1:
                        alpha*=2
                    else:
                        wolfe_flag=True
                bisect_count+=1
            x1=x0+alpha*delta_x
            f1=f(x1,*args,**kwargs)
            nf1=npla.norm(f1)
        if inform:
            F.append(nf1)
        newt_count+=1
    if nf1>errtol and npla.norm(x1-x0)>errtol:
        if inform:
            print("Unable to find root or critical point after %i iterations"%newt_count)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return None
    else:
        if inform:
            if nf1<=errtol:
                print("Root found after %i iterations"%newt_count)
                print("||f(x)||_2 = %0.6f"%nf1)
            else:
                print("Extremum found after %i iterations"%newt_count)
                print("||f(x)||_2 = %0.6f"%nf1)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return x1

def minimize(f,x0,args=[],kwargs={},errtol=1e-6,maxlevel=100,mode="bfgs",adapt=True,inform=False):
    #This method performs a BFGS approximation of the minimum of the function 
    #f:R->R, using vector x0 as it's initial guess. Variable errtol sets the
    #minimum norm that the function gradient can take on before an approximate
    #of the minimum is made. Variable maxlevel determines the maximum number of
    #iterations the method can perform before it is forced to end.
    #If variable mode is set to "newton_raphson", the inverse Hessian is
    #computed every iteration. If variable mode is set to "bfgs", the
    #inverse Hessian is approximated using the BFGS method (default).
    #If variable adapt is set to True, the method uses the weak Wolfe 
    #conditions to damp (or accelerate) the Newton-Raphson iteration to coerce
    #global converge to a minimum or minimum of the function's gradient.
    #If variable inform is set to True, the method relays information on 
    #it's converge behavior to the user. 
    
    df=lambda X:gtb.differentiate(f,X,args=args,kwargs=kwargs)
    try:
        df0=df(x0,*args,**kwargs)
    except Exception:
        print("Error: unable to initialize algorithm using inputted vector x0")
        return x0
    ndf0=abs(df0)
    
    if ("int" in type(x0).__name__) or ("float" in type(x0).__name__):
        if ("int" not in type(df0).__name__) and ("float" not in type(df0).__name__):
            if inform:
                print("Error: unable to apply Newton-Raphson iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Newton-Raphson iteration to this function")
        return None
    
    mode=mode.lower()
    if ("newt" in mode) or ("raph" in mode):
        mode="newton_raphson"    
        try:
            dfinv=1.0/gtb.differentiate(df,x0,args=args,kwargs=kwargs)
        except Exception:
            dfinv=0.0
    else:
        dfinv=1.0
        mode="bfgs"
    
    alpha=1.0
    min_flag=(ndf0<=errtol)
    extreme_flag=False
    newt_count=0
    if inform:
        F=[ndf0]
    while not min_flag and not extreme_flag and newt_count<maxlevel:
        delta_x=-dfinv*df0
        if adapt and delta_x*df0<0:
            if mode=="newton_raphson":
                alpha=line_search(f,x0,delta_x,args=args,kwargs=kwargs,mode="wolfe")
            else:
                alpha=line_search(f,x0,delta_x,args=args,kwargs=kwargs,mode="goldstein")
        x1=x0+alpha*delta_x
        df1=df(x1)
        ndf1=abs(df1)
        if mode=="newton_raphson":
            try:
                dfinv=1.0/gtb.differentiate(df,x0,args=args,kwargs=kwargs)
            except Exception:
                try:
                    dfinv=delta_x/df1-df0
                except Exception:
                    dfinv=0.0
        else:
            try:
                dfinv=delta_x/df1-df0
            except Exception:
                try:
                    dfinv=1.0/gtb.differentiate(df,x0,args=args,kwargs=kwargs)
                except Exception:
                    dfinv=0.0

        if inform:
            F.append(ndf1)
        min_flag=(ndf1<=errtol)
        extreme_flag=(abs(x1-x0)<=errtol)
        x0=x1
        df0=df1
        ndf0=ndf1
        newt_count+=1
        
    if not min_flag and not extreme_flag:
        if inform:
            print("Unable to find minimum after %i iterations"%newt_count)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Gradient Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Gradient Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return None
    else:
        if inform:
            if min_flag:
                print("Minimum found after %i iterations"%newt_count)
                print("|diff_f(x)| = %0.6f"%ndf0)
            else:
                print("Extremum of gradient found after %i iterations"%newt_count)
                print("|diff_f(x)| = %0.6f"%ndf0)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Gradient Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Gradient Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return x0

def multi_minimize(f,x0,args=[],kwargs={},errtol=1e-6,maxlevel=100,mode="bfgs",adapt=True,inform=False):
    #This method performs a BFGS approximation of the minimum of the function 
    #f:R^n->R, using vector x0 as it's initial guess. Variable errtol sets the
    #minimum norm that the function gradient can take on before an approximate
    #of the minimum is made. Variable maxlevel determines the maximum number of
    #iterations the method can perform before it is forced to end.
    #If variable mode is set to "newton_raphson", the inverse Hessian is
    #computed every iteration. If variable mode is set to "bfgs", the
    #inverse Hessian is approximated using the BFGS method (default).
    #If variable adapt is set to True, the method uses the weak Wolfe 
    #conditions to damp (or accelerate) the Newton-Raphson iteration to coerce
    #global converge to a minimum or minimum of the function's gradient.
    #If variable inform is set to True, the method relays information on 
    #it's converge behavior to the user.
    
    x0=np.asarray(x0)
    df=lambda X:mtb.grad(f,X,args=args,kwargs=kwargs)
    try:
        df0=df(x0,*args,**kwargs)
    except Exception:
        print("Error: unable to initialize algorithm using inputted vector x0")
        return x0
    ndf0=npla.norm(df0)
    
    if ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        if ("list" in type(df0).__name__) or ("tuple" in type(df0).__name__) or ("ndarray" in type(df0).__name__):
            if len(x0)==len(df0):
                dim=len(x0)
            else:
                if inform:
                    print("Error: unable to apply Newton-Raphson iteration to f:R^%i->R^%i"%(len(x0),len(df0)))
                return None
        else:
            if inform:
                print("Error: unable to apply Newton-Raphson iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Newton-Raphson iteration to this function")
        return None
    
    mode=mode.lower()
    if ("newt" in mode) or ("raph" in mode):
        mode="newton_raphson"    
        try:
            dfinv=npla.inv(mtb.jacobian(df,x0,args=args,kwargs=kwargs))
        except Exception:
            dfinv=np.zeros(dim,dim)
    else:
        dfinv=np.identity(dim)
        if "broyden" in mode:
            mode="broyden"
        elif "dfp" in mode:
            mode="dfp"
        elif ("sr" in mode) or ("1" in mode):
            mode="sr-1"
        else:
            mode="bfgs"
    
    alpha=1.0
    min_flag=(ndf0<=errtol)
    extreme_flag=False
    newt_count=0
    if inform:
        F=[ndf0]
    while not min_flag and not extreme_flag and newt_count<maxlevel:
        delta_x=-np.dot(dfinv,df0)
        if adapt and np.inner(delta_x,df0)<0:
            if mode=="newton_raphson":
                alpha=line_search(f,x0,delta_x,args=args,kwargs=kwargs,mode="wolfe")
            else:
                alpha=line_search(f,x0,delta_x,args=args,kwargs=kwargs,mode="goldstein")
        x1=x0+alpha*delta_x
        df1=df(x1)
        ndf1=npla.norm(df1)
        if mode=="newton_raphson":
            try:
                dfinv=npla.inv(mtb.jacobian(df,x0,args=args,kwargs=kwargs))
            except Exception:
                try:
                    delta_df=df1-df0
                    inner_df=np.inner(delta_df,delta_x)
                    temp_matrix=np.dot(dfinv,np.outer(delta_df,delta_x))
                    dfinv=dfinv+(np.inner(delta_df,delta_x)+np.inner(delta_df,np.dot(dfinv,delta_df)))/inner_df**2*np.outer(delta_x,delta_x)-(temp_matrix+np.transpose(temp_matrix))/inner_df
                except Exception:
                    dfinv=np.zeros((dim,dim))
        else:
            try:
                delta_df=df1-df0
                if mode=="broyden":
                    res1=np.dot(dfinv,delta_df)
                    dfinv=dfinv+np.outer(delta_x-res1,np.dot(delta_x,dfinv))/np.inner(delta_x,res1)
                elif mode=="dfp":
                    res1=np.dot(dfinv,delta_df)
                    dfinv=dfinv+np.outer(delta_x,delta_x)/np.inner(delta_x,delta_df)-np.outer(res1,res1)/np.inner(delta_df,res1)
                elif mode=="sr-1":
                    res1=delta_x-np.dot(dfinv,delta_df)
                    dfinv=dfinv+np.outer(res1,res1)/np.inner(res1,delta_df)
                else:
                    res1=np.inner(delta_df,delta_x)
                    res2=np.dot(dfinv,np.outer(delta_df,delta_x))
                    dfinv=dfinv+(np.inner(delta_df,delta_x)+np.inner(delta_df,np.dot(dfinv,delta_df)))*np.outer(delta_x,delta_x)/res1**2-(res2+np.transpose(res2))/res1
            except Exception:
                try:
                    dfinv=npla.inv(mtb.jacobian(df,x0,args=args,kwargs=kwargs))
                except Exception:
                    dfinv=np.zeros((dim,dim))

        if inform:
            F.append(ndf1)
        min_flag=(ndf1<=errtol)
        extreme_flag=(npla.norm(x1-x0)<=errtol)
        x0=x1.copy()
        df0=df1.copy()
        ndf0=ndf1
        newt_count+=1
        
    if not min_flag and not extreme_flag:
        if inform:
            print("Unable to find minimum after %i iterations"%newt_count)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Gradient Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Gradient Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return None
    else:
        if inform:
            if min_flag:
                print("Minimum found after %i iterations"%newt_count)
                print("||grad_f(x)||_2 = %0.6f"%ndf0)
            else:
                print("Extremum of gradient found after %i iterations"%newt_count)
                print("||grad_f(x)||_2 = %0.6f"%ndf0)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Gradient Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Gradient Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return x0

def minimize_constrained(f,x0,h=lambda X:[],g=lambda X:[],errtol=1e-6,zerotol=1e-8,maxlevel=100,inform=False,mode="broyden"):
    #This method performs an active set SQP algorithm of the minimum of the function 
    #f:R^n->R, using vector x0 as it's initial guess under equality and inequality constraints.
    #Variable errtol sets the minimum norm that the function gradient can take on
    #before an approximate of the minimum is made. Variable zerotol removes roundoff errors.
    #Variable maxlevel determines the maximum number of
    #iterations the method can perform before it is forced to end.
    #If variable mode is set to "newton_raphson", the inverse Hessian is
    #computed every iteration. If variable mode is set to "bfgs", the
    #inverse Hessian is approximated using the BFGS method (default).
    #If variable adapt is set to True, the method uses the weak Wolfe 
    #conditions to damp (or accelerate) the Newton-Raphson iteration to coerce
    #global converge to a minimum or minimum of the function's gradient.
    #If variable inform is set to True, the method relays information on 
    #it's converge behavior to the user. 
    if ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        x0=np.asarray(x0)
        n=len(x0)
        try:
            if ("int" not in type(f(x0)).__name__) and ("float" not in type(f(x0)).__name__):
                if inform:
                    print("Error: objective function f is not a R^%i->R function"%n)
                return None
        except Exception:
            if inform:
                print("Error: unable to initialize algorithm by applying objective function f to point x0")
            return None
        
        try:
            if ("list" in type(h(x0)).__name__) or ("tuple" in type(h(x0)).__name__) or ("ndarray" in type(h(x0)).__name__):
                m_e=len(h(x0))
            else:
                if inform:
                    print("Error: equality constraint function h is not a R^%i->R^m_e function"%n)
                return None
        except Exception:
            if inform:
                print("Error: unable to initialize algorithm by applying equality constraint function h to point x0")
            return None
        
        try:
            if ("list" in type(g(x0)).__name__) or ("tuple" in type(g(x0)).__name__) or ("ndarray" in type(g(x0)).__name__):
                m_i=len(g(x0))
            else:
                if inform:
                    print("Error: inequality constraint function g is not a R^%i->R^m_i function"%n)
                return None
        except Exception:
            if inform:
                print("Error: unable to initialize algorithm by applying inequality constraint function g to point x0")
            return None
    else:
        if inform:
            print("Error: unable to initialize SQP algorithm with inputted point x0.")
        return None
    
    mode=mode.lower()
    if "b" in mode and "f" in mode and "g" in mode:
        mode="bfgs"
    elif "b" in mode:
        mode="broyden"
    elif "d" in mode and "f" in mode and "p" in mode:
        mode="dfp"
    elif "s" in mode and "r" in mode and "1" in mode:
        mode="sr1"
    else:
        mode="newton_raphson"
    
    if m_i>0:
        res_G=g(x0)
        A=[i for i in range(m_i) if res_G[i]>=-abs(zerotol)]
        m_A=len(A)
        I=[i for i in range(m_i) if res_G[i]<-abs(zerotol)]
        m_I=len(I)
    else:
        A=[]
        m_A=0
        I=[]
        m_I=0
    
    def g_A(x):
        res_G=g(x)
        return np.array([res_G[i] for i in A])
    
    def H(x):
        try:
            B=np.zeros((n+m_e+m_A,n+m_e+m_A))
            B[:n,:n]=mtb.jacobian(lambda X:mtb.grad(f,X),x)
            if m_e>0:
                B[n:n+m_e,:n]=mtb.jacobian(h,x)
                B[:n,n:n+m_e]=B[n:n+m_e,:n].T
            if m_A>0:
                B[n+m_e:,:n]=mtb.jacobian(g_A,x)
                B[:n,n+m_e:]=B[n+m_e:,:n].T
            return B
        except Exception:
            return np.identity(n+m_e+m_A)
    
    def delta_f(x):
        delta_f=np.zeros(n+m_e+m_A)
        delta_f[:n]=mtb.grad(f,x)
        if m_e>0:
            delta_f[n:n+m_e]=h(x)
        if m_A>0:
            delta_f[n+m_e:]=g_A(x)
        return delta_f
        
    if mode=="newton_raphson":
        B=H(x0)
    else:
        B=npla.inv(H(x0))
    D_F=delta_f(x0)
    if inform:
        F=[f(x0)]
    sqp_bool=True
    count=0
    while sqp_bool and count<maxlevel:
        if mode=="newton_raphson":
            try:
                res_d=npla.solve(B,-1.0*D_F)
            except npla.LinAlgError as err:
                if "Singular matrix" in str(err):
                    res_d=-1.0*D_F/npla.norm(D_F)
                else:
                    raise
        else:
            res_d=-np.dot(B,D_F)
        delta_x=np.zeros(n+m_e+m_i)
        delta_x[:n+m_e]=res_d[:n+m_e]
        for i in range(m_A):
            delta_x[n+m_e+A[i]]+=res_d[n+m_e+i]
        
        if npla.norm(delta_x[:n])<=errtol:
            sqp_bool=False
            if m_A>0:
                for i in range(m_A):
                    if delta_x[n+m_e+A[i]]<-abs(zerotol):
                        I.append(A[i])
                        m_I+=1
                        A=A[:i]+A[i+1:]
                        m_A-=1
                        sqp_bool=True
                        if mode=="newton_raphson":
                            B=H(x0)
                        else:
                            B=npla.inv(H(x0))
                        D_F=delta_f(x0)
                        break
        else:
            alpha=1.0
            if m_I>0:
                res_G=g(x0)
                res_JG=mtb.jacobian(g,x0)
                for i in I:
                    res1=res_G[i]
                    res2=np.inner(res_JG[i,:],delta_x[:n])
                    if abs(res2)>abs(zerotol):
                        res=-res1/res2
                    else:
                        res=1.0
                    if 0<res<=alpha:
                        alpha=res
            x0=x0+alpha*delta_x[:n]
            changed_bool=True
            if m_i>0:
                res_G=g(x0)
                temp_A=A.copy()
                temp_m_A=m_A
                A=[i for i in range(m_i) if res_G[i]>=-abs(zerotol)]
                m_A=len(A)
                if temp_m_A!=m_A or temp_A!=A:
                    changed_bool=False
                I=[i for i in range(m_i) if res_G[i]<-abs(zerotol)]
                m_I=len(I)
            if mode=="newton_raphson":
                B=H(x0)
                D_F=delta_f(x0)
            else:
                if changed_bool:
                    if mode=="bfgs":
                        try:
                            res_d=alpha*res_d
                            temp_D_F=D_F.copy()
                            D_F=delta_f(x0)
                            res1=D_F-temp_D_F
                            res2=np.inner(res_d,res1)
                            res3=np.dot(B,np.outer(res1,res_d))
                            B=B+(res2+np.inner(res1,np.dot(B,res1)))*np.outer(res_d,res_d)/(res2**2)-(res3+res3.T)/res2
                        except Exception:
                            B=npla.inv(H(x0))
                            D_F=delta_f(x0)
                    elif mode=="broyden":
                        try:
                            res_d=alpha*res_d
                            temp_D_F=D_F.copy()
                            D_F=delta_f(x0)
                            res1=D_F-temp_D_F
                            res2=np.dot(B,res1)
                            B=B+np.dot(np.outer((res-res2),res),B)/np.inner(res,res2)
                        except Exception:
                            B=npla.inv(H(x0))
                            D_F=delta_f(x0)
                    elif mode=="dfp":
                        try:
                            res_d=alpha*res_d
                            temp_D_F=D_F.copy()
                            D_F=delta_f(x0)
                            res1=D_F-temp_D_F
                            res2=np.dot(B,res)
                            B=B+np.outer(res,res)/np.inner(res,res1)-np.outer(res2,res2)/np.inner(res1,np.dot(B,res1))
                        except Exception:
                            B=npla.inv(H(x0))
                            D_F=delta_f(x0)
                    elif mode=="sr1":
                        try:
                            res_d=alpha*res_d
                            temp_D_F=D_F.copy()
                            D_F=delta_f(x0)
                            res1=D_F-temp_D_F
                            res2=res-np.dot(B,res)
                            B=B+np.outer(res2,res)/np.inner(res2,res)
                        except Exception:
                            B=npla.inv(H(x0))
                            D_F=delta_f(x0)
                    else:
                        B=npla.inv(H(x0))
                        D_F=delta_f(x0)
                else:
                    B=npla.inv(H(x0))
                    D_F=delta_f(x0)
        if inform:
            F.append(f(x0))
        count+=1
        
    if npla.norm(delta_x[:n])<=errtol and all(delta_x[n+m_e:]>=-abs(zerotol)):
        if inform:
            print("Minimum found after %i iterations using %s"%(count,mode))
            print("f(x) = %0.6f"%f(x0))
            ax1=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax1.plot([i for i in range(len(F))],F)
            ax1.set_title("Function Value per Iteration"%mode,fontdict=font)
            ax1.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax1.set_ylabel("Function Value",fontsize=16)
            ax1.xaxis.set_tick_params(labelsize=16)
            ax1.yaxis.set_tick_params(labelsize=16)
        return x0
    else:
        if inform:
            if count==maxlevel:
                print("Unable to find minimum after %i iterations using %s"%(count,mode))
            else:
                print("Converged to suboptimal point after %i iterations using %s"%(count,mode))
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Value per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Value",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return None
