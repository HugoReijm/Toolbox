import numpy as np

def simplex(A,b,c,inequalities=None,mode="maximize",inform=False):
    #This method solves the optimization problem
    #"maximize c^Tx subject to Ax<=b and x>=0" using the Simplex Method,
    #assuming the set of feasible solutions is convex. Variable <inform> allows
    #for the user to see diagnostic data of the method.
    
    if not isinstance(A,np.ndarray):
        try:
            A=np.array(A)
        except Exception:
            print("Error: Matrix A of incompatible type.")
            return None
    if not isinstance(b,np.ndarray):
        try:
            b=np.array(b)
        except Exception:
            print("Error: Vector b of incompatible type")
            return None
    if not isinstance(c,np.ndarray):
        try:
            c=np.array(c)
        except Exception:
            print("Vector c of incompatible type")
            return None
    if A.shape[1]!=c.shape[0] or A.shape[0]!=b.shape[0]:
        print("A,b, and c of incompatible sizes")
        return None
    
    n=c.shape[0]
    m=0
    if isinstance(inequalities,(list,tuple,np.ndarray)):
        inequalities=np.array(inequalities)
        for i in range(min(A.shape[0],inequalities.shape[0])):
            try:
                inequalities[i]=inequalities[i].lower()
                if ">" in inequalities[i] or "great" in inequalities[i]:
                    inequalities[i]=">"
                    m+=1
                #elif "=" in inequalities[i] or "equal" in inequalities[i]:
                #    inequalities[i]="="
                    #SLIGHT ERROR: MUST FIRST BE ON A FEASIBLE SOLUTION
                else:
                    inequalities[i]="<"
                    m+=1
            except Exception:
                inequalities[i]="<"
                m+=1
        m+=A.shape[0]-inequalities.shape[0]
        inequalities=np.hstack((inequalities,["<" for i in range(inequalities.shape[0],A.shape[0])]))
    else:
        m=A.shape[0]
        inequalities=["<" for i in range(A.shape[0])]
    
    d=np.hstack((c,np.zeros(m)))
    if isinstance(mode,str):
        if "min" in mode.lower():
            r1=np.hstack((d,0))
        else:
            r1=np.hstack((-d,0))
    else:
        r1=np.hstack((-d,0))
    
    
    A=np.hstack((A,np.zeros((A.shape[0],m))))
    col_count=n
    for i in range(A.shape[0]):
        if inequalities[i]==">":
            A[i,col_count]=-1.0
            col_count+=1
        elif inequalities[i]=="<":
            A[i,col_count]=1.0
            col_count+=1
    r2=np.hstack((A,np.array([b]).T))
    S=np.vstack((r1,r2))
    
    iter_count=0
    pivot=True
    while pivot:
        pivot_col=-1
        max_cost=0
        for j in range(n+m):
            if S[0,j]<max_cost:
                pivot_col=j
                max_cost=S[0,j]
                #break
        if pivot_col==-1:
            pivot=False
            break
        
        pivot_row=-1
        min_ratio=np.inf
        for i in range(1,S.shape[0]):
            if S[i,pivot_col]!=0:
                res=S[i,-1]/S[i,pivot_col]
                if 0<res<min_ratio:
                    pivot_row=i
                    min_ratio=res
        if pivot_row==-1:
            pivot=False
            break
        
        if S[pivot_row,pivot_col]!=1.0:
            S[pivot_row,:]=S[pivot_row,:]/S[pivot_row,pivot_col]
            S[pivot_row,pivot_col]=1.0
        
        for i in range(S.shape[0]):
            if i!=pivot_row and S[i,pivot_col]!=0:
                S[i,:]=S[i,:]-S[pivot_row,:]*S[i,pivot_col]
                S[i,pivot_col]=0.0
        
        iter_count+=1
    
    sol=np.zeros(S.shape[1]-1)
    for j in range(n+m):
        basis_bool=True
        one_pos=-1
        for i in range(1,S.shape[0]):
            if S[i,j]!=0.0:
                if S[i,j]!=1.0:
                    basis_bool=False
                    break
                else:
                    if one_pos!=-1:
                        basis_bool=False
                        break
                    else:
                        one_pos=i
        if basis_bool and one_pos!=-1:
            sol[j]=S[one_pos,-1]
    
    if inform:
        print("Optimal Variable Values:")
        for j in range(n):
            if sol[j]!=0.0:
                print("x_%i = %0.6f"%(j+1,sol[j]))
        print("Optimal Slack Variable Values:")
        for i in range(n,n+m):
            if sol[i]!=0.0:
                print("x_%i = %0.6f"%(i+1,sol[i]))
        print("Maximal Value: %0.6f"%S[0,-1])
        print("Number of Iterations: %i"%iter_count)

    return sol[:n]

def nelder_mead(f,x0,args=[],kwargs={},errtol=1e-6,maxlevel=100,inform=False):
    class simplex(object):
        def __init__(self,V,f):
            self.V=V.copy()
            self.f=f
            self.F=np.array([self.f(v,*args,**kwargs) for v in V])
            
        def order(self):
            index=np.argsort(self.F)
            self.V=self.V[index]
            self.F=self.F[index]
            self.C=np.mean(self.V[:-1],axis=0)
            self.FC=self.f(self.C,*args,**kwargs)
            
        def reflect(self,a=1.0):
            v=self.C+a*(self.C-self.V[-1])
            fv=self.f(v,*args,**kwargs)
            if fv<=self.F[0]:
                self.expand(v,fv)
            elif fv<=self.F[-2]:
                self.V[-1]=v.copy()
                self.F[-1]=fv
            elif fv<self.F[-1]:
                self.contract(v,fv)
            else:
                self.contract(v,fv,inner=True)
        
        def expand(self,vr,fvr,a=2.0):
            v=self.C+a*(self.C-self.V[-1])
            fv=self.f(v,*args,**kwargs)
            if fv<=self.F[0]:
                #self.expand(v,fv,a=2*a)
                self.V[-1]=v.copy()
                self.F[-1]=fv
            else:
                self.V[-1]=vr.copy()
                self.F[-1]=fvr
        
        def contract(self,vr,fvr,a=0.5,inner=False):
            if inner:
                v=self.C-a*(self.C-self.V[-1])
            else:
                v=self.C+a*(self.C-self.V[-1])
            fv=self.f(v,*args,**kwargs)
            if (inner and fv<=self.F[-1]) or (not inner and fv<=fvr):
                self.V[-1]=v.copy()
                self.F[-1]=fv
            else:
                self.shrink()
        
        def shrink(self,a=0.5):
            self.V[1:]=self.V[1:]-a*(self.V[1:]-self.V[0])
            self.F[1:]=[self.f(v,*args,**kwargs) for v in V[1:]]
    
    if ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        x0=np.asarray(x0)
        dim=len(x0)
        try:
            f0=f(x0,*args,**kwargs)
            if ("int" not in type(f0).__name__) and ("float" not in type(f0).__name__):
                if inform:
                    print("Error: unable to apply Nelder-Mead iteration to this function")
                return None
        except Exception:
            if inform:
                print("Error: unable to apply Nelder-Mead iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Nelder-Mead iteration to this function")
        return None
    
    V=[x0]
    for i in range(dim):
        res=np.zeros(dim)
        res[i]=1.0
        V.append(x0+res)
    V=np.array(V)
    s=simplex(V,f)
    std=np.std(s.F)
    count=0
    if inform:
        plotline=[f(np.mean(s.V,axis=0),*args,**kwargs)]
    while std>errtol and count<maxlevel:
        s.order()
        s.reflect()
        std=np.std(s.F)
        if inform:
            plotline.append(s.F[0])
        count+=1
    
    if inform:
        print("Solution after %i iterations"%count)
        print("F(x)=%0.6f"%s.F[0])
        ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
        ax.plot([i for i in range(len(plotline))],plotline)
        ax.set_title("Simplex Min Function Value per Iteration",fontdict=font)
        ax.set_xlabel("Iteration",fontsize=16,rotation=0)
        ax.set_ylabel("Function Value",fontsize=16)
        ax.xaxis.set_tick_params(labelsize=16)
        ax.yaxis.set_tick_params(labelsize=16)
    return s.V[0]

def line_search(f,x0,delta_x,args=[],kwargs={},c1=1e-4,c2=0.9,mode="Wolfe"):
    mode=mode.lower()
    if "wolf" in mode:
        mode="wolfe"
    else:
        mode="goldstein"
    f0=f(x0)
    a=0.0
    alpha=1.0
    b=np.inf
    bisect_count=0
    cond_flag=False
    if ("int" in type(x0).__name__) or ("float" in type(x0).__name__):
        grad_f0=differentiate(f,x0,args=args,kwargs=kwargs)
        while not cond_flag and bisect_count<100:
            try:
                res1=f(x0+alpha*delta_x)
                if res1>f0+c1*alpha*delta_x*grad_f0:
                    b=alpha
                    alpha=0.5*(a+b)
                else:
                    res2=differentiate(f,x0+alpha*delta_x,args=args,kwargs=kwargs)
                    if ((mode=="wolfe" and res2<c2*grad_f0)
                            or (mode=="goldstein" and res1<f0+(1-c1)*alpha*delta_x*grad_f0)):
                        a=alpha
                        if b<np.inf:
                            alpha=0.5*(a+b)
                        else:
                            alpha=2*a
                    else:
                        cond_flag=True
            except Exception:
                b=alpha
                alpha=0.5*(a+b)
            bisect_count+=1
    elif ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        grad_f0=grad(f,x0,args=args,kwargs=kwargs)
        while not cond_flag and bisect_count<100:
            try:
                res1=f(x0+alpha*delta_x)
                res2=grad(f,x0+alpha*delta_x,args=args,kwargs=kwargs)
                if ((mode=="wolfe" and res1>f0+c1*alpha*np.dot(delta_x,grad_f0))
                        or (mode=="goldstein" and res1>f0+c1*alpha*np.dot(grad_f0,delta_x))):
                    b=alpha
                    alpha=0.5*(a+b)
                elif ((mode=="wolfe" and np.dot(delta_x,res2)<c2*np.dot(delta_x,grad_f0))
                        or (mode=="goldstein" and res1<f0+(1-c1)*alpha*np.dot(delta_x,grad_f0))):
                    a=alpha
                    if b<np.inf:
                        alpha=0.5*(a+b)
                    else:
                        alpha=2*a
                else:
                    cond_flag=True
            except Exception:
                b=alpha
                alpha=0.5*(a+b)
            bisect_count+=1
    else:
        return None
    return alpha

def minimize(f,x0,args=[],kwargs={},errtol=1e-6,maxlevel=100,mode="bfgs",adapt=True,inform=False):
    #This method performs a BFGS approximation of the minimum of the function 
    #f:R->R, using vector x0 as it's initial guess. Variable errtol sets the
    #minimum norm that the function gradient can take on before an approximate
    #of the minimum is made. Variable maxlevel determines the maximum number of
    #iterations the method can perform before it is forced to end.
    #If variable mode is set to "newton_raphson", the inverse Hessian is
    #computed every iteration. If variable mode is set to "bfgs", the
    #inverse Hessian is approximated using the BFGS method (default).
    #If variable adapt is set to True, the method uses the weak Wolfe 
    #conditions to damp (or accelerate) the Newton-Raphson iteration to coerce
    #global converge to a minimum or minimum of the function's gradient.
    #If variable inform is set to True, the method relays information on 
    #it's converge behavior to the user. 
    
    from toolbox.matrixtoolbox import jacobian,grad
    
    df=lambda X:differentiate(f,X,args=args,kwargs=kwargs)
    try:
        df0=df(x0,*args,**kwargs)
    except Exception:
        print("Error: unable to initialize algorithm using inputted vector x0")
        return x0
    ndf0=abs(df0)
    
    if ("int" in type(x0).__name__) or ("float" in type(x0).__name__):
        if ("int" not in type(df0).__name__) and ("float" not in type(df0).__name__):
            if inform:
                print("Error: unable to apply Newton-Raphson iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Newton-Raphson iteration to this function")
        return None
    
    mode=mode.lower()
    if ("newt" in mode) or ("raph" in mode):
        mode="newton_raphson"    
        try:
            dfinv=1.0/differentiate(df,x0,args=args,kwargs=kwargs)
        except Exception:
            dfinv=0.0
    else:
        dfinv=1.0
        mode="bfgs"
    
    alpha=1.0
    min_flag=(ndf0<=errtol)
    extreme_flag=False
    newt_count=0
    if inform:
        F=[ndf0]
    while not min_flag and not extreme_flag and newt_count<maxlevel:
        delta_x=-dfinv*df0
        if adapt and delta_x*df0<0:
            if mode=="newton_raphson":
                alpha=line_search(f,x0,delta_x,args=args,kwargs=kwargs,mode="wolfe")
            else:
                alpha=line_search(f,x0,delta_x,args=args,kwargs=kwargs,mode="goldstein")
        x1=x0+alpha*delta_x
        df1=df(x1)
        ndf1=abs(df1)
        if mode=="newton_raphson":
            try:
                dfinv=1.0/differentiate(df,x0,args=args,kwargs=kwargs)
            except Exception:
                try:
                    dfinv=delta_x/df1-df0
                except Exception:
                    dfinv=0.0
        else:
            try:
                dfinv=delta_x/df1-df0
            except Exception:
                try:
                    dfinv=1.0/differentiate(df,x0,args=args,kwargs=kwargs)
                except Exception:
                    dfinv=0.0

        if inform:
            F.append(ndf1)
        min_flag=(ndf1<=errtol)
        extreme_flag=(abs(x1-x0)<=errtol)
        x0=x1
        df0=df1
        ndf0=ndf1
        newt_count+=1
        
    if not min_flag and not extreme_flag:
        if inform:
            print("Unable to find minimum after %i iterations"%newt_count)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Gradient Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Gradient Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return None
    else:
        if inform:
            if min_flag:
                print("Minimum found after %i iterations"%newt_count)
                print("|diff_f(x)| = %0.6f"%ndf0)
            else:
                print("Extremum of gradient found after %i iterations"%newt_count)
                print("|diff_f(x)| = %0.6f"%ndf0)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Gradient Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Gradient Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return x0

def multi_minimize(f,x0,args=[],kwargs={},errtol=1e-6,maxlevel=100,mode="bfgs",adapt=True,inform=False):
    #This method performs a BFGS approximation of the minimum of the function 
    #f:R^n->R, using vector x0 as it's initial guess. Variable errtol sets the
    #minimum norm that the function gradient can take on before an approximate
    #of the minimum is made. Variable maxlevel determines the maximum number of
    #iterations the method can perform before it is forced to end.
    #If variable mode is set to "newton_raphson", the inverse Hessian is
    #computed every iteration. If variable mode is set to "bfgs", the
    #inverse Hessian is approximated using the BFGS method (default).
    #If variable adapt is set to True, the method uses the weak Wolfe 
    #conditions to damp (or accelerate) the Newton-Raphson iteration to coerce
    #global converge to a minimum or minimum of the function's gradient.
    #If variable inform is set to True, the method relays information on 
    #it's converge behavior to the user. 
    
    from toolbox.matrixtoolbox import jacobian,grad
    
    x0=np.asarray(x0)
    df=lambda X:grad(f,X,args=args,kwargs=kwargs)
    try:
        df0=df(x0,*args,**kwargs)
    except Exception:
        print("Error: unable to initialize algorithm using inputted vector x0")
        return x0
    ndf0=npla.norm(df0)
    
    if ("list" in type(x0).__name__) or ("tuple" in type(x0).__name__) or ("ndarray" in type(x0).__name__):
        if ("list" in type(df0).__name__) or ("tuple" in type(df0).__name__) or ("ndarray" in type(df0).__name__):
            if len(x0)==len(df0):
                dim=len(x0)
            else:
                if inform:
                    print("Error: unable to apply Newton-Raphson iteration to f:R^%i->R^%i"%(len(x0),len(df0)))
                return None
        else:
            if inform:
                print("Error: unable to apply Newton-Raphson iteration to this function")
            return None
    else:
        if inform:
            print("Error: unable to apply Newton-Raphson iteration to this function")
        return None
    
    mode=mode.lower()
    if ("newt" in mode) or ("raph" in mode):
        mode="newton_raphson"    
        try:
            dfinv=npla.inv(jacobian(df,x0,args=args,kwargs=kwargs))
        except Exception:
            dfinv=np.zeros(dim,dim)
    else:
        dfinv=np.identity(dim)
        if "broyden" in mode:
            mode="broyden"
        elif "dfp" in mode:
            mode="dfp"
        elif ("sr" in mode) or ("1" in mode):
            mode="sr-1"
        else:
            mode="bfgs"
    
    alpha=1.0
    min_flag=(ndf0<=errtol)
    extreme_flag=False
    newt_count=0
    if inform:
        F=[ndf0]
    while not min_flag and not extreme_flag and newt_count<maxlevel:
        delta_x=-np.dot(dfinv,df0)
        if adapt and np.inner(delta_x,df0)<0:
            if mode=="newton_raphson":
                alpha=line_search(f,x0,delta_x,args=args,kwargs=kwargs,mode="wolfe")
            else:
                alpha=line_search(f,x0,delta_x,args=args,kwargs=kwargs,mode="goldstein")
        x1=x0+alpha*delta_x
        df1=df(x1)
        ndf1=npla.norm(df1)
        if mode=="newton_raphson":
            try:
                dfinv=npla.inv(jacobian(df,x0,args=args,kwargs=kwargs))
            except Exception:
                try:
                    delta_df=df1-df0
                    inner_df=np.inner(delta_df,delta_x)
                    temp_matrix=np.dot(dfinv,np.outer(delta_df,delta_x))
                    dfinv=dfinv+(np.inner(delta_df,delta_x)+np.inner(delta_df,np.dot(dfinv,delta_df)))/inner_df**2*np.outer(delta_x,delta_x)-(temp_matrix+np.transpose(temp_matrix))/inner_df
                except Exception:
                    dfinv=np.zeros((dim,dim))
        else:
            try:
                delta_df=df1-df0
                if mode=="broyden":
                    res1=np.dot(dfinv,delta_df)
                    dfinv=dfinv+np.outer(delta_x-res1,np.dot(delta_x,dfinv))/np.inner(delta_x,res1)
                elif mode=="dfp":
                    res1=np.dot(dfinv,delta_df)
                    dfinv=dfinv+np.outer(delta_x,delta_x)/np.inner(delta_x,delta_df)-np.outer(res1,res1)/np.inner(delta_df,res1)
                elif mode=="sr-1":
                    res1=delta_x-np.dot(dfinv,delta_df)
                    dfinv=dfinv+np.outer(res1,res1)/np.inner(res1,delta_df)
                else:
                    res1=np.inner(delta_df,delta_x)
                    res2=np.dot(dfinv,np.outer(delta_df,delta_x))
                    dfinv=dfinv+(np.inner(delta_df,delta_x)+np.inner(delta_df,np.dot(dfinv,delta_df)))*np.outer(delta_x,delta_x)/res1**2-(res2+np.transpose(res2))/res1
            except Exception:
                try:
                    dfinv=npla.inv(jacobian(df,x0,args=args,kwargs=kwargs))
                except Exception:
                    dfinv=np.zeros((dim,dim))

        if inform:
            F.append(ndf1)
        min_flag=(ndf1<=errtol)
        extreme_flag=(npla.norm(x1-x0)<=errtol)
        x0=x1.copy()
        df0=df1.copy()
        ndf0=ndf1
        newt_count+=1
        
    if not min_flag and not extreme_flag:
        if inform:
            print("Unable to find minimum after %i iterations"%newt_count)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Gradient Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Gradient Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return None
    else:
        if inform:
            if min_flag:
                print("Minimum found after %i iterations"%newt_count)
                print("||grad_f(x)||_2 = %0.6f"%ndf0)
            else:
                print("Extremum of gradient found after %i iterations"%newt_count)
                print("||grad_f(x)||_2 = %0.6f"%ndf0)
            ax=plt.figure(figsize=(graphsize,graphsize)).add_subplot(111)
            ax.plot([i for i in range(len(F))],F)
            ax.set_title("Function Gradient Norm per Iteration",fontdict=font)
            ax.set_xlabel("Iteration",fontsize=16,rotation=0)
            ax.set_ylabel("Function Gradient Norm",fontsize=16)
            ax.xaxis.set_tick_params(labelsize=16)
            ax.yaxis.set_tick_params(labelsize=16)
        return x0
